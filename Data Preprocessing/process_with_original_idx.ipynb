{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"process_with_original_idx.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"y_znVmlYOYm5"},"source":["import os, re, json\n","import spacy\n","from functools import partial\n","import pandas as pd\n","from tqdm import tqdm\n","import xml.etree.ElementTree as ET\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","nlp = spacy.load('en_core_web_sm', disable=['textcat'])\n","\n","class ECBMentionsDoc:\n","\n","    # Problematic tokens in the dataset\n","    # From the CDLM repo\n","    error_tokens = [('31_10ecbplus.xml', 979),\n","                  ('9_3ecbplus.xml', 30),\n","                  ('9_4ecbplus.xml', 32)]\n","\n","\n","    def __init__(self, doc_path, doc_name, topic_id):\n","        self.doc_path = doc_path\n","        self.doc_name = doc_name\n","        self.topic_id = topic_id\n","        self.mentions_fields = {}\n","        self.mention_cluster_info = {}\n","        self.relation_source_target = {}\n","        self.relation_ids = {}\n","        self.relation_tag = {}\n","        self.event_singleton_idx = int(1E8)\n","        self.entity_singleton_idx = int(2E8)\n","        self.entity_mentions = []\n","        self.event_mentions = []\n","        self.tagged_event_tokens = {}\n","        self.tagged_entity_tokens = {}\n","        self.doc_token_texts = {}\n","        self.b_open, self.b_close = \"{\", \"}\"\n","        self.prev_wrap, self.prev_tag_id = '', ''\n","        self.tag_is_opened = False\n","\n","\n","\n","    def parse_xml(self):\n","        # Start parsing\n","        self.root = ET.parse(self.doc_path).getroot()\n","\n","        # Set all mention ids from the full document for both event and entity mentions\n","        self.set_all_marked_mentions()\n","\n","        # Set all cross doc ids\n","        self.set_cross_doc_mentions()\n","\n","        # Creates both arrays containing all event and entity mention info\n","        self.compute_event_entity_mentions()\n","\n","        # Parses all the actual tokens from the current document into a dict we can use\n","        self.set_doc_texts()\n","\n","\n","\n","    def set_doc_texts(self):\n","        '''\n","        Example text:\n","        <token t_id=\"53\" sentence=\"4\" number=\"1\">Williams</token>\n","        <token t_id=\"54\" sentence=\"4\" number=\"2\">,</token>\n","        <token t_id=\"55\" sentence=\"4\" number=\"3\">the</token>\n","        <token t_id=\"56\" sentence=\"4\" number=\"4\">swimming</token>\n","        <token t_id=\"57\" sentence=\"4\" number=\"5\">champion</token>\n","        <token t_id=\"58\" sentence=\"4\" number=\"6\">turned</token>\n","        <token t_id=\"59\" sentence=\"4\" number=\"7\">actress</token>\n","        '''\n","        prev_sent_id = -1\n","        for token in self.root.findall('token'):\n","            token_id = int(token.get('t_id'))\n","            \n","            # A few tokens per should not be used\n","            if (self.doc_name, token_id) not in self.error_tokens:\n","                # Parse actual token text in the right format\n","                token_text = token.text.replace('�', '').strip()\n","                sent_id = int(token.get('sentence'))\n","\n","                # word_id_sent = token.get('number') # word index per sentence\n","                token_info = (token_text, sent_id)\n","                \n","                # Write data for sentence reconstruction\n","                if (prev_sent_id > -1) and (sent_id != prev_sent_id):\n","                    prev_token_info = self.doc_token_texts[prev_token_id]\n","                    self.doc_token_texts[prev_token_id] = (prev_token_info[0] + \" [EOS]\", prev_token_info[1])\n","\n","                self.doc_token_texts[token_id] = token_info\n","                prev_sent_id = sent_id\n","                prev_token_id = token_id\n","\n","\n","    def compute_event_entity_mentions(self):\n","        \n","        # Loop through all mentions of the current document\n","        for m_id, mention in self.mentions_fields.items():\n","\n","            # For this specific mention check if's a source by checking if it maps to a target\n","            # Since the dict containts {source_mention_id: target_mention_id}\n","            target_id = self.relation_source_target.get(m_id, None)\n","\n","            # If it's just a source_id with no second target_id in it's cluster;\n","            # then we know that this event or enntity mention has to be a singleton\n","            if target_id is None:\n","                if mention['event']:\n","                    cluster_id = self.event_singleton_idx\n","                    self.event_singleton_idx += 1\n","                else:\n","                    cluster_id = self.entity_singleton_idx\n","                    self.entity_singleton_idx += 1\n","\n","                # cluster_id =  'Singleton_' + file_name + '_' + m_id\n","                cluster_desc = ''\n","            else:\n","                # Relation id is basically the cluster's id to identify a cluser\n","                r_id = self.relation_ids[target_id]\n","                tag = self.relation_tag[target_id] # E.g. CROSS_DOC_COREF\n","                \n","                # Only within doc link\n","                if tag.startswith('INTRA'):\n","                    # Entity and event mentions may have the same intra cluster id \n","                    suffix = '1' if mention['event'] else '0' \n","                    cluster_id =  int(r_id + suffix)\n","                else:\n","                    # Grab the cluster info dict from the mention clusters we created\n","                    target_cluster_info = self.mention_cluster_info[target_id]\n","\n","                    # E.g. ACT16236402809085484\n","                    target_cluster_id_str = target_cluster_info['cluster_id']\n","\n","                    # We grab all the integers from this string to construct an int we can use\n","                    cluster_id = int(target_cluster_id_str[3:])\n","\n","                # e.g. t4_swimming_skills\n","                cluster_desc = self.mention_cluster_info[target_id]['cluster_desc']\n","\n","\n","            # Now that we retrieved the cluster id and description for this mention;\n","            # We can update the mention dict we create before with this and append;\n","            # it to the entities correpsonding group -> Event or Entity mention\n","            mention_info = mention.copy()\n","            mention_info[\"cluster_id\"] = cluster_id\n","            mention_info[\"cluster_desc\"] = cluster_desc\n","            event = mention_info.pop(\"event\")\n","            if event:\n","                self.event_mentions.append(mention_info)\n","            else:\n","                self.entity_mentions.append(mention_info)\n","\n","\n","    def set_cross_doc_mentions(self):\n","        '''\n","        Example part to parse:\n","        <CROSS_DOC_COREF r_id=\"22306\" note=\"ACT16195873839112917\">\n","            <source m_id=\"28\" />\n","            <source m_id=\"34\" />\n","            <target m_id=\"60\" />\n","        </CROSS_DOC_COREF>\n","        '''\n","\n","        # Relation -> Cross doc relation\n","        for relation in self.root.find('Relations'):\n","            \n","            # Last element of each cluster is 'target'\n","            target_mention_id = relation[-1].attrib['m_id']\n","            \n","            # All the other elements are of type 'source'\n","            source_tags = relation[:-1]\n","\n","            # Set a mapping from coref source id to it's master target\n","            for source_tag in source_tags:\n","                source_mention_id = source_tag.attrib['m_id']\n","                self.relation_source_target[source_mention_id] = target_mention_id\n","\n","            \n","            # Save tag 'CROSS_DOC_COREF' \n","            self.relation_tag[target_mention_id] = relation.tag\n","\n","            # Save the target mention id to cross doc id entries\n","            self.relation_ids[target_mention_id] = relation.attrib['r_id']\n","\n","\n","\n","\n","\n","    def set_all_marked_mentions(self):\n","        '''\n","        Example part to parse:\n","        <ACTION_ASPECTUAL m_id=\"53\">\n","            <token_anchor t_id=\"186\"/>\n","        </ACTION_ASPECTUAL>\n","        <ACTION_OCCURRENCE m_id=\"50\">\n","            <token_anchor t_id=\"179\"/>\n","            <token_anchor t_id=\"180\"/>\n","            <token_anchor t_id=\"181\"/>\n","        </ACTION_OCCURRENCE>\n","        '''\n","\n","        # Store our results\n","        subtopic = '0' if 'plus' in self.doc_name else '1'\n","\n","        for mention in self.root.find('Markables'):\n","            m_id = mention.attrib['m_id']\n","\n","            if 'RELATED_TO' not in mention.attrib:\n","\n","                # ACTION or NEG is an event mention \n","                is_event_mention = mention.tag.startswith('ACT') or mention.tag.startswith('NEG')\n","                \n","                # Grab all token ids under current Markable tag\n","                tokens_ids = [int(term.attrib['t_id']) for term in mention]\n","\n","                if len(tokens_ids) == 0:\n","                    print(ET.tostring(mention, encoding='unicode'))\n","                    continue\n","\n","                # print(is_event_mention, tokens_ids)\n","\n","                # Indexing our sentences also starts at 0\n","                token_sent_index = tokens_ids[0]\n","                sent_id = self.root[token_sent_index].attrib['sentence']\n","\n","                # Construct the actual mention text, e.g. \"Barack Obama\"\n","                # NOTE: We -1 the token id itself, since they started indexing at 1 and map starts at 0\n","                mention_word_tokens = ' '.join(list(map(lambda x: self.root[x-1].text, tokens_ids)))\n","\n","                lemmas, tags = [], []\n","                for tok in nlp(mention_word_tokens):\n","                    lemmas.append(tok.lemma_)\n","                    tags.append(tok.tag_)\n","                \n","                self.mentions_fields[m_id] = {\n","                    \"doc_id\": self.doc_name,\n","                    \"topic\": self.topic_id,\n","                    \"subtopic\": self.doc_name.split('_')[0] + '_' + subtopic,\n","                    \"m_id\": m_id,\n","                    \"sentence_id\" : int(sent_id),\n","                    \"tokens_ids\": tokens_ids,\n","                    \"tokens\": mention_word_tokens,\n","                    \"tags\": ' '.join(tags),\n","                    \"lemmas\": ' '.join(lemmas),\n","                    \"event\": is_event_mention\n","                }\n","            else:\n","                self.mention_cluster_info[m_id] = {\n","                    \"cluster_id\": mention.attrib.get('instance_id', ''),\n","                    \"cluster_desc\": mention.attrib['TAG_DESCRIPTOR']\n","                }\n","\n","\n","\n","    def get_word_tags(self, mentions):\n","        # Format to df for easy indexing\n","        mentions_df = pd.DataFrame(mentions)\n","\n","        # To tag each group of words\n","        tag_id = 0\n","        tagged_mention_tokens = {}\n","        matched_cluster_ids = {}\n","\n","        # E.g. [10] or [111, 112]\n","        for _, row in mentions_df[['tokens_ids', 'cluster_id']].iterrows():\n","            cluster_id = row['cluster_id']\n","            token_ids = row['tokens_ids']\n","\n","            # This takes of formatting cluster ids to normal tag ids\n","            # e.g. 15737539387899295 -> 1 and 15743207473194727 -> 2\n","            if cluster_id not in matched_cluster_ids:\n","                matched_cluster_ids[cluster_id] = tag_id\n","                tag_id += 1\n","            \n","            word_tag_id = matched_cluster_ids[cluster_id]\n","\n","            # So all tokens in a tagged part of a string have the same tag_id\n","            for token_id in token_ids:\n","                tagged_mention_tokens[token_id] = word_tag_id\n","\n","        return tagged_mention_tokens\n","\n","\n","\n","    def get_wrap_span_word(self, word, token_id, token_tag_opened, max_token_id):\n","        \n","        tag_id = self.tagged_entity_tokens[token_id]\n","        next_token_id = token_id + 1\n","        word_wrap = word\n","        \n","        # If span only has 1 token, then fully wrap it -> Check if next has same tag\n","        if next_token_id not in self.tagged_entity_tokens.keys():\n","            if self.b_open in self.prev_wrap:\n","                word_wrap = f\"{word} {self.b_close}\"\n","                self.tag_is_opened = False\n","\n","            else:\n","                word_wrap = f\"{self.b_open}tag_id:{tag_id} {word} {self.b_close}\"\n","            \n","        else:\n","            next_tag_id = self.tagged_entity_tokens[next_token_id]\n","            \n","            # Check this is the start of a span\n","            if self.b_open not in self.prev_wrap:\n","                \n","                # Previous token is not the start and the tag ends here so self contained\n","                if tag_id != next_tag_id:\n","                    word_wrap = f\"{self.b_open}tag_id:{tag_id} {word} {self.b_close}\"\n","                \n","                \n","                # Check token itself is the start of a span or middle\n","                if tag_id != self.prev_tag_id and not self.tag_is_opened:\n","           \n","                    # This means we are starting a multi-token span\n","                    word_wrap = f\"{self.b_open}tag_id:{tag_id} {word}\"\n","                    self.tag_is_opened = True\n","         \n","            \n","            # Token tag has already opened, so either closing or a middle token\n","            else:\n","                \n","                # print(tag_id, next_tag_id)\n","                # If the next tag is different it means this tag ends\n","                if tag_id != next_tag_id:\n","                    word_wrap = f\"{word} {self.b_close}\"\n","                    self.tag_is_opened = False\n","\n","                \n","                # print(\" -> Next tag is the same as this one!\")\n","                # A middle word so no closing tag\n","                word_wrap = word\n","                \n","            self.prev_tag_id = tag_id\n","            \n","        return word_wrap                        \n","                \n","\n","    def clear_url(self):\n","        \n","        no_space_doc =  self.original_text.replace(\" \", \"\")\n","\n","        print(no_space_doc)\n","        \n","\n","    def compute_formatted_entity_doc(self, b_open = \"{\", b_close=\"}\"):\n","        # Maps each tagged token id to the tag_id we want to use\n","        # e.g. {token_id: tag_id} {14: 3, 15: 3}\n","        self.tagged_entity_tokens = self.get_word_tags(self.entity_mentions)\n","\n","        # To store the actual words/characters of the documents\n","        self.formatted_doc_tokens = []\n","        self.clean_doc_tokens = []\n","        clean_tokens_v2 = []\n","        token_tag_opened = 1\n","        max_token_id = len(self.doc_token_texts.keys()) + 1 # Starts at 1\n","        prev_wrap = ''\n","        self.original_text = ''\n","\n","        for token_id, (word, sent_id) in  self.doc_token_texts.items():\n","            clean_tokens_v2.append(word)\n","            current_wrap = word\n","            # Check if the current token is part of a tagged span\n","            if token_id in self.tagged_entity_tokens:\n","                current_wrap = self.get_wrap_span_word(word, token_id, token_tag_opened, max_token_id)\n","                \n","            next_token_id = token_id\n","            prev_token_id = token_id if token_id == 1 else token_id - 1\n","\n","            \n","            if next_token_id != max_token_id:\n","                token_exceptions =  [\",\", \".\", \"'\", \"’\", \"\\\"\", \"/\", \"“\",\"www.\", \"http\", \":\", \"com/\",\"-\"]\n","                \n","                \n","                prev_word = self.doc_token_texts[prev_token_id][0]\n","                next_word = self.doc_token_texts[next_token_id][0]\n","                \n","                # print(word)\n","                # If next token is aplhanumer, then it means it's word so it needs a space\n","                if word not in token_exceptions and next_word not in token_exceptions:                    \n","                    self.formatted_doc_tokens.append(' ')\n","                    self.clean_doc_tokens.append(' ')\n","                # else:\n","                    # print(f\"Next word '{next_word[:1]}' is NOT alpha\")\n","            \n","            # print(word, current_wrap)\n","            self.formatted_doc_tokens.append(current_wrap)\n","            self.clean_doc_tokens.append(word)\n","            self.prev_wrap = current_wrap\n","        \n","        # Convert list to 1 string\n","        self.doc_prompt = ''.join(word for word in self.formatted_doc_tokens)\n","        self.original_text = ''.join(word for word in self.clean_doc_tokens)\n","        original_textv2 = TreebankWordDetokenizer().detokenize(clean_tokens_v2)\n","        # print(clean_tokens_v2)\n","        # print(original_textv2)\n","        # print(\"-----clean v2 custom---\\n\")\n","        # print(untokenize(clean_tokens_v2))\n","        # # Remove url\n","        # if self.original_text.startswith(\"http\"):\n","        #     self.clear_url()\n","        \n","\n","    # http://www.ws.com/May 2, 2013.. -> http://www.ws.com/ May 2, 2013.. or May 2, 2013..\n","    def split_url_on_month(self, match, keep_start_url):\n","        matched_url = match.group()\n","        months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n","\n","        # Minimum amount of characters for an url plus month string\n","        if len(matched_url) > 10 and any([x in matched_url for x in months]):\n","            \n","            # Compile a regex with each month as option\n","            regexPattern = '|'.join(map(re.escape, months))\n","            \n","            # Split the string by 1 of the months, also keeping the matched month itself\n","            matches = re.split(f\"({regexPattern})\", matched_url, 1)\n","            new_url, month = matches[0], matches[1]   \n","            \n","            # So we want to keep the original url            \n","            if keep_start_url:\n","            \n","                # Return the url with the space in between the month\n","                return f\"{new_url} {month}\"\n","            \n","            return month\n","\n","        \n","        # So we want to keep the original url            \n","        if keep_start_url:\n","            return matched_url\n","        \n","        # We can just skip the url altogether\n","        return ''            \n","    \n","\n","    # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n","    # https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py\n","    def untokenize(self, words, keep_start_url=False):\n","        \"\"\"\n","        Untokenizing a text undoes the tokenizing operation, restoring\n","        punctuation and spaces to the places that people expect them to be.\n","        Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","        except for line breaks.\n","        \"\"\"\n","        text = ' '.join(words)\n","        text = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","        text = text.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n","        text = re.sub(r' ([.,:;?!%]+)([ \\'`])', r\"\\1\\2\", text)\n","        text = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", text)\n","        text = text.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n","            \"can not\", \"cannot\")\n","        text = text.replace(\" ` \", \" '\").replace(\" -\", \"-\").replace(\"- \", \"-\")\n","        text = text.replace(\" ,\", \",\").replace(' /',  '/').replace('/ ',  '/')\n","        text = text.replace(\" ’ s\", \"'s\").replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n","        text = text.replace(\" ’ s\", \"'s\").replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n","        text = text.replace(\"www. \", \"www.\").replace(\". com\", \".com\").replace(\" ”\", \"”\")\n","        text = text.replace(\" _ \", \"_\")\n","        text = text.replace(\"p. m.\", \"p.m.\").replace(\"a. m.\", \"a.m.\")\n","\n","        # Regex to match even amount of \", because removing trailing or start space;\n","        # Will also remove any characters before and after quotes start.\n","        # So we need to match the even amount, see: https://stackoverflow.com/a/53436792/8970591\n","        # Inspiration for regex: https://stackoverflow.com/questions/14906492/how-can-whitespace-be-trimmed-from-a-regex-capture-group\n","        quote_regex = '\\\\\"\\s?([^\\]]*?)\\s?\\\\\"'\n","        text = re.sub(quote_regex, '\\\"'+r'\\1'+'\\\"' , text)\n","                \n","        # A lot of articles start with an url in the as the source it came from\n","        # So we can optionally get rid of this to get a cleaner text for text generation\n","        first_url_regex = '^(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'        \n","        pattern = re.compile(first_url_regex)\n","        \n","        # Partials can be used to make new derived functions that have some input parameters pre-assigned\n","        re_sub_callback = partial(self.split_url_on_month, keep_start_url=keep_start_url)\n","        \n","        # Sometime the first word, which is a month, can be captured by our regex\n","        # So we want to split on this character to we can keep the month\n","        text = re.sub(pattern, re_sub_callback, text, 1)\n","\n","\n","        return text.strip()\n","\n","\n","\n","    def format_doc_and_mentions(self, keep_start_url=True):\n","        doc_tokens = [el[1][0] for el in self.doc_token_texts.items()]\n","    \n","        plain_text = ' '.join(word for word in doc_tokens)\n","        clean_text = self.untokenize(doc_tokens, keep_start_url)\n","        # print(doc_tokens)\n","        return clean_text\n","\n","    def get_clusters(self, mentions):\n","        clusters = {}\n","        \n","        for i, mention in enumerate(mentions):\n","            cluster_id = mention['cluster_id']\n","\n","            # Create empty list entry if not existent\n","            if cluster_id not in clusters:\n","                clusters[cluster_id] = []\n","\n","\n","            clusters[cluster_id] += mention[\"tokens_ids\"]\n","\n","        return clusters\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQ5EK-RNOyjO","executionInfo":{"status":"ok","timestamp":1639360532426,"user_tz":-60,"elapsed":15382,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"17c367a7-3c11-440d-89ca-0f2a33596f27"},"source":["import random\n","from google.colab import drive\n","import pickle\n","drive.mount('/content/drive',force_remount=True)\n","\n","# Make sure to click \"Add shortcut to drive\" for the \"Coref-for-GPT\" folder\n","gdrive_dir_path = \"/content/drive/MyDrive/Coref-for-GPT\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["local_path = \"\"\n","\n","# Change this to \"local_path\" if you run the notebook locally\n","root_path = gdrive_dir_path"],"metadata":{"id":"wJhHSLfNf6ov"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the ecb data\n","ecb_path = f\"{root_path}/Data/ECB+/\"\n","ecb_gold_path = f\"{root_path}/Data/ECB+/gold/\""],"metadata":{"id":"33JfsYZgf97J"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w45bsUUaPn93"},"source":["# Load gold train, dev, test entity dataset\n","def load_gold_conll(ds_type):\n","    file_path = f\"{ecb_gold_path}/mentions/%s_entities.json\"%(ds_type) \n","    with open (file_path) as f:\n","        file = json.load(f)\n","    return file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C84oMAqiQa5Y","executionInfo":{"status":"ok","timestamp":1639360883745,"user_tz":-60,"elapsed":1281,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"c744e304-25a8-4843-8571-c486266aa823"},"source":["dev = load_gold_conll(\"dev\")\n","train = load_gold_conll(\"train\")\n","print(len(dev), len(train))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1476 4758\n"]}]},{"cell_type":"code","metadata":{"id":"YOc7mGIEV72R"},"source":["def get_info_for_docs(ds, dir_path):\n","    docs = []\n","    # info: (text, mentions, clusters)\n","    info_list = []\n","\n","    for doc_info in tqdm(ds):\n","        doc_name = doc_info[\"doc_id\"]\n","        if len(docs) == 0:\n","            prev_doc = \"\"\n","        else:\n","            prev_doc = docs.pop()\n","\n","        if prev_doc == doc_name:\n","            docs.append(doc_name)\n","        else:\n","            if prev_doc:\n","                docs.append(prev_doc)\n","            docs.append(doc_name)\n","            topic, _ = doc_name.split(\"_\")\n","            ecb_path = dir_path+ \"ECB+/\"+ f\"{topic}/{doc_name}\"\n","            try:\n","                ecb_mention_doc = ECBMentionsDoc(ecb_path, doc_name, 2)\n","                ecb_mention_doc.parse_xml()\n","                mentions = ecb_mention_doc.entity_mentions\n","            \n","                clusters = ecb_mention_doc.get_clusters(mentions)\n","                text = ecb_mention_doc.format_doc_and_mentions(keep_start_url=True)\n","            except:\n","                print(doc_name)\n","                continue\n","            info_list.append((text, mentions, clusters))\n","      \n","    return docs,info_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7C4fsp5j18j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639360928409,"user_tz":-60,"elapsed":222,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"091412ca-6d9b-408d-a4c9-2a21fbb29995"},"source":["dev_docs, dev_info = get_info_for_docs(dev, ecb_path)\n","print(len(dev_docs))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1476/1476 [00:00<00:00, 9562.99it/s] "]},{"output_type":"stream","name":"stdout","text":["35_10ecb.xml\n","35_11ecbplus.xml\n","35_10ecbplus.xml\n","35_1ecb.xml\n","35_1ecbplus.xml\n","35_2ecbplus.xml\n","35_2ecb.xml\n","35_3ecbplus.xml\n","35_3ecb.xml\n","35_5ecbplus.xml\n","35_4ecb.xml\n","35_5ecb.xml\n","35_4ecbplus.xml\n","35_7ecb.xml\n","35_7ecbplus.xml\n","35_6ecbplus.xml\n","35_6ecb.xml\n","35_8ecbplus.xml\n","35_9ecb.xml\n","35_8ecb.xml\n","35_9ecbplus.xml\n","34_10ecb.xml\n","34_11ecb.xml\n","34_10ecbplus.xml\n","34_13ecb.xml\n","34_12ecb.xml\n","34_11ecbplus.xml\n","34_12ecbplus.xml\n","34_15ecb.xml\n","34_1ecb.xml\n","34_16ecb.xml\n","34_14ecb.xml\n","34_2ecb.xml\n","34_2ecbplus.xml\n","34_3ecb.xml\n","34_1ecbplus.xml\n","34_5ecbplus.xml\n","34_4ecbplus.xml\n","34_3ecbplus.xml\n","34_4ecb.xml\n","34_6ecb.xml\n","34_6ecbplus.xml\n","34_7ecb.xml\n","34_7ecbplus.xml\n","34_8ecbplus.xml\n","34_9ecb.xml\n","34_8ecb.xml\n","34_9ecbplus.xml\n","18_10ecb.xml\n","18_10ecbplus.xml\n","18_11ecb.xml\n","18_11ecbplus.xml\n","18_13ecb.xml\n","18_12ecb.xml\n","18_16ecb.xml\n","18_1ecbplus.xml\n","18_14ecb.xml\n","18_1ecb.xml\n","18_15ecb.xml\n","18_3ecb.xml\n","18_3ecbplus.xml\n","18_2ecb.xml\n","18_2ecbplus.xml\n","18_4ecb.xml\n","18_5ecbplus.xml\n","18_5ecb.xml\n","18_4ecbplus.xml\n","18_6ecbplus.xml\n","18_6ecb.xml\n","18_8ecb.xml\n","18_7ecbplus.xml\n","18_7ecb.xml\n","18_9ecb.xml\n","18_8ecbplus.xml\n","18_9ecbplus.xml\n","21_10ecb.xml\n","21_10ecbplus.xml\n","21_11ecbplus.xml\n","21_12ecb.xml\n","21_11ecb.xml\n","21_12ecbplus.xml\n","21_13ecbplus.xml\n","21_1ecbplus.xml\n","21_2ecb.xml\n","21_14ecbplus.xml\n","21_2ecbplus.xml\n","21_1ecb.xml\n","21_3ecbplus.xml\n","21_3ecb.xml\n","21_4ecbplus.xml\n","21_4ecb.xml\n","21_5ecbplus.xml\n","21_6ecb.xml\n","21_6ecbplus.xml\n","21_5ecb.xml\n","21_7ecb.xml\n","21_7ecbplus.xml\n","21_9ecb.xml\n","21_8ecb.xml\n","21_8ecbplus.xml\n","21_9ecbplus.xml\n","23_10ecb.xml\n","23_2ecb.xml\n","23_1ecb.xml\n","23_1ecbplus.xml\n","23_10ecbplus.xml\n","23_11ecbplus.xml\n","23_3ecb.xml\n","23_2ecbplus.xml\n","23_3ecbplus.xml\n","23_4ecb.xml\n","23_5ecbplus.xml\n","23_6ecb.xml\n","23_5ecb.xml\n","23_4ecbplus.xml\n","23_7ecbplus.xml\n","23_6ecbplus.xml\n","23_7ecb.xml\n","23_8ecb.xml\n","23_9ecb.xml\n","23_8ecbplus.xml\n","23_9ecbplus.xml\n","2_11ecb.xml\n","2_1ecb.xml\n","2_11ecbplus.xml\n","2_10ecbplus.xml\n","2_3ecb.xml\n","2_2ecbplus.xml\n","2_3ecbplus.xml\n","2_1ecbplus.xml\n","2_2ecb.xml\n","2_4ecb.xml\n","2_5ecbplus.xml\n","2_5ecb.xml\n","2_4ecbplus.xml\n","2_6ecb.xml\n","2_6ecbplus.xml\n","2_7ecb.xml\n","2_7ecbplus.xml\n","2_8ecb.xml\n","2_8ecbplus.xml\n","2_9ecbplus.xml\n","2_9ecb.xml\n","5_10ecb.xml\n","5_12ecb.xml\n","5_13ecb.xml\n","5_11ecb.xml\n","5_10ecbplus.xml\n","5_14ecb.xml\n","5_2ecb.xml\n","5_1ecbplus.xml\n","5_1ecb.xml\n","5_4ecb.xml\n","5_3ecbplus.xml\n","5_3ecb.xml\n","5_2ecbplus.xml\n","5_5ecbplus.xml\n","5_6ecb.xml\n","5_4ecbplus.xml\n","5_5ecb.xml\n","5_6ecbplus.xml\n","5_7ecbplus.xml\n","5_9ecb.xml\n","5_8ecbplus.xml\n","5_9ecbplus.xml\n","5_7ecb.xml\n","12_10ecb.xml\n","12_11ecbplus.xml\n","12_13ecb.xml\n","12_10ecbplus.xml\n","12_11ecb.xml\n","12_12ecb.xml\n","12_17ecb.xml\n","12_15ecb.xml\n","12_14ecb.xml\n","12_16ecb.xml\n","12_1ecb.xml\n","12_2ecb.xml\n","12_18ecb.xml\n","12_1ecbplus.xml\n","12_19ecb.xml\n","12_3ecbplus.xml\n","12_4ecb.xml\n","12_2ecbplus.xml\n","12_3ecb.xml\n","12_5ecbplus.xml\n","12_4ecbplus.xml\n","12_6ecb.xml\n","12_5ecb.xml\n","12_7ecb.xml\n","12_8ecb.xml\n","12_8ecbplus.xml\n","12_7ecbplus.xml\n","12_6ecbplus.xml\n","12_9ecb.xml\n","12_9ecbplus.xml\n","196\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"kI6gg5TIQutX"},"source":["devset = dict(zip(dev_docs, dev_info))\n","file_path = ecb_path + \"processed/dev_with_original_index.json\"\n","with open(file_path, 'w') as f:\n","    json.dump(devset,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLN5DA0rOhNd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639361160014,"user_tz":-60,"elapsed":627,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"5f998021-383e-45c5-bb8d-3db8494bac0d"},"source":["train_docs, train_info = get_info_for_docs(train, ecb_input_path)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 1035/4758 [00:00<00:00, 10335.84it/s]"]},{"output_type":"stream","name":"stdout","text":["20_11ecbplus.xml\n","20_1ecb.xml\n","20_2ecb.xml\n","20_1ecbplus.xml\n","20_4ecb.xml\n","20_3ecbplus.xml\n","20_2ecbplus.xml\n","20_3ecb.xml\n","20_4ecbplus.xml\n","20_5ecbplus.xml\n","20_5ecb.xml\n","20_6ecbplus.xml\n","20_8ecbplus.xml\n","20_7ecbplus.xml\n","20_9ecbplus.xml\n","32_10ecbplus.xml\n","32_1ecb.xml\n","32_11ecbplus.xml\n","32_1ecbplus.xml\n","32_2ecb.xml\n","32_3ecb.xml\n","32_3ecbplus.xml\n","32_2ecbplus.xml\n","32_5ecb.xml\n","32_4ecbplus.xml\n","32_5ecbplus.xml\n","32_4ecb.xml\n","32_7ecb.xml\n","32_8ecb.xml\n","32_6ecb.xml\n","32_6ecbplus.xml\n","32_7ecbplus.xml\n","32_9ecbplus.xml\n","32_8ecbplus.xml\n","33_11ecbplus.xml\n","33_1ecb.xml\n","33_10ecbplus.xml\n","33_3ecb.xml\n","33_2ecb.xml\n","33_1ecbplus.xml\n","33_2ecbplus.xml\n","33_3ecbplus.xml\n","33_4ecb.xml\n","33_4ecbplus.xml\n","33_9ecbplus.xml\n","33_5ecb.xml\n","33_5ecbplus.xml\n","33_8ecbplus.xml\n","33_6ecbplus.xml\n","33_7ecbplus.xml\n","7_10ecbplus.xml\n","7_11ecb.xml\n","7_11ecbplus.xml\n","7_10ecb.xml\n","7_1ecb.xml\n","7_1ecbplus.xml\n","7_2ecbplus.xml\n","7_2ecb.xml\n","7_4ecbplus.xml\n","7_3ecb.xml\n","7_3ecbplus.xml\n","7_5ecb.xml\n","7_7ecb.xml\n","7_6ecb.xml\n","7_5ecbplus.xml\n","7_6ecbplus.xml\n","7_9ecbplus.xml\n","7_7ecbplus.xml\n","7_8ecb.xml\n","7_8ecbplus.xml\n","7_9ecb.xml\n","11_11ecb.xml\n","11_1ecb.xml\n","11_10ecb.xml\n","11_1ecbplus.xml\n","11_3ecbplus.xml\n","11_2ecb.xml\n","11_3ecb.xml\n","11_2ecbplus.xml\n","11_6ecb.xml\n","11_5ecbplus.xml\n","11_4ecb.xml\n","11_5ecb.xml\n","11_4ecbplus.xml\n","11_7ecb.xml\n","11_9ecb.xml\n","11_8ecb.xml\n","27_10ecb.xml\n","27_10ecbplus.xml\n","27_11ecbplus.xml\n","27_13ecb.xml\n","27_11ecb.xml\n","27_12ecb.xml\n","27_15ecb.xml\n","27_16ecb.xml\n","27_14ecb.xml\n","27_2ecb.xml\n","27_17ecb.xml\n","27_1ecbplus.xml\n","27_1ecb.xml\n","27_2ecbplus.xml\n","27_4ecbplus.xml\n","27_4ecb.xml\n","27_3ecb.xml\n","27_3ecbplus.xml\n","27_5ecb.xml\n","27_6ecb.xml\n","27_5ecbplus.xml\n","27_6ecbplus.xml\n","27_7ecb.xml\n","27_7ecbplus.xml\n","27_9ecb.xml\n","27_8ecb.xml\n","27_8ecbplus.xml\n","27_9ecbplus.xml\n","9_12ecbplus.xml\n","9_11ecbplus.xml\n","9_10ecbplus.xml\n","9_10ecb.xml\n","9_2ecbplus.xml\n","9_2ecb.xml\n","9_1ecbplus.xml\n","9_1ecb.xml\n","9_13ecbplus.xml\n","9_4ecbplus.xml\n","9_3ecbplus.xml\n","9_4ecb.xml\n","9_3ecb.xml\n","9_5ecbplus.xml\n","9_6ecb.xml\n","9_6ecbplus.xml\n","9_5ecb.xml\n","9_8ecb.xml\n","9_7ecbplus.xml\n","9_7ecb.xml\n","9_9ecb.xml\n","9_8ecbplus.xml\n","9_9ecbplus.xml\n","29_10ecb.xml\n","29_11ecb.xml\n","29_10ecbplus.xml\n","29_12ecbplus.xml\n","29_11ecbplus.xml\n","29_1ecb.xml\n","29_13ecbplus.xml\n","29_2ecb.xml\n","29_2ecbplus.xml\n","29_1ecbplus.xml\n","29_4ecb.xml\n","29_4ecbplus.xml\n","29_5ecb.xml\n","29_3ecb.xml\n","29_3ecbplus.xml\n","29_6ecbplus.xml\n","29_6ecb.xml\n","29_5ecbplus.xml\n","29_7ecb.xml\n","29_9ecb.xml\n","29_7ecbplus.xml\n","29_8ecb.xml\n","29_8ecbplus.xml\n","29_9ecbplus.xml\n","6_1ecbplus.xml\n","6_11ecbplus.xml\n","6_10ecbplus.xml\n","6_1ecb.xml\n","6_2ecb.xml\n","6_2ecbplus.xml\n","6_4ecb.xml\n","6_3ecb.xml\n","6_3ecbplus.xml\n","6_5ecbplus.xml\n","6_6ecb.xml\n","6_4ecbplus.xml\n","6_5ecb.xml\n","6_7ecbplus.xml\n","6_8ecb.xml\n","6_6ecbplus.xml\n","6_7ecb.xml\n","6_9ecbplus.xml\n","6_8ecbplus.xml\n","6_9ecb.xml\n","16_10ecbplus.xml\n","16_11ecbplus.xml\n","16_1ecb.xml\n","16_3ecbplus.xml\n","16_2ecbplus.xml\n","16_1ecbplus.xml\n","16_3ecb.xml\n","16_2ecb.xml\n","16_6ecbplus.xml\n","16_7ecbplus.xml\n","16_5ecbplus.xml\n","16_4ecbplus.xml\n","16_8ecbplus.xml\n","16_9ecbplus.xml\n","1_11ecb.xml\n","1_10ecb.xml\n","1_11ecbplus.xml\n","1_10ecbplus.xml\n","1_13ecb.xml\n","1_12ecbplus.xml\n","1_14ecb.xml\n","1_12ecb.xml\n","1_13ecbplus.xml\n","1_14ecbplus.xml\n","1_15ecb.xml\n","1_17ecb.xml\n","1_16ecbplus.xml\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 2953/4758 [00:00<00:00, 8560.37it/s]"]},{"output_type":"stream","name":"stdout","text":["1_15ecbplus.xml\n","1_18ecbplus.xml\n","1_19ecb.xml\n","1_18ecb.xml\n","1_17ecbplus.xml\n","1_1ecbplus.xml\n","1_20ecbplus.xml\n","1_19ecbplus.xml\n","1_1ecb.xml\n","1_3ecb.xml\n","1_21ecbplus.xml\n","1_2ecb.xml\n","1_2ecbplus.xml\n","1_4ecbplus.xml\n","1_5ecb.xml\n","1_3ecbplus.xml\n","1_4ecb.xml\n","1_5ecbplus.xml\n","1_7ecb.xml\n","1_6ecbplus.xml\n","1_7ecbplus.xml\n","1_6ecb.xml\n","1_8ecb.xml\n","1_9ecb.xml\n","1_8ecbplus.xml\n","1_9ecbplus.xml\n","28_10ecb.xml\n","28_10ecbplus.xml\n","28_11ecb.xml\n","28_12ecbplus.xml\n","28_11ecbplus.xml\n","28_13ecb.xml\n","28_12ecb.xml\n","28_2ecb.xml\n","28_1ecbplus.xml\n","28_1ecb.xml\n","28_4ecb.xml\n","28_3ecb.xml\n","28_2ecbplus.xml\n","28_3ecbplus.xml\n","28_5ecb.xml\n","28_5ecbplus.xml\n","28_4ecbplus.xml\n","28_6ecb.xml\n","28_6ecbplus.xml\n","28_7ecbplus.xml\n","28_9ecb.xml\n","28_8ecb.xml\n","28_8ecbplus.xml\n","28_7ecb.xml\n","28_9ecbplus.xml\n","26_10ecb.xml\n","26_10ecbplus.xml\n","26_11ecbplus.xml\n","26_12ecb.xml\n","26_11ecb.xml\n","26_13ecb.xml\n","26_1ecbplus.xml\n","26_2ecb.xml\n","26_1ecb.xml\n","26_2ecbplus.xml\n","26_4ecb.xml\n","26_3ecb.xml\n","26_3ecbplus.xml\n","26_5ecbplus.xml\n","26_4ecbplus.xml\n","26_5ecb.xml\n","26_7ecb.xml\n","26_6ecb.xml\n","26_8ecb.xml\n","26_8ecbplus.xml\n","26_6ecbplus.xml\n","26_7ecbplus.xml\n","26_9ecb.xml\n","26_9ecbplus.xml\n","19_10ecb.xml\n","19_10ecbplus.xml\n","19_11ecb.xml\n","19_1ecb.xml\n","19_15ecb.xml\n","19_14ecb.xml\n","19_12ecb.xml\n","19_11ecbplus.xml\n","19_2ecbplus.xml\n","19_2ecb.xml\n","19_1ecbplus.xml\n","19_3ecb.xml\n","19_4ecbplus.xml\n","19_3ecbplus.xml\n","19_4ecb.xml\n","19_5ecb.xml\n","19_5ecbplus.xml\n","19_6ecb.xml\n","19_7ecb.xml\n","19_6ecbplus.xml\n","19_8ecb.xml\n","19_7ecbplus.xml\n","19_8ecbplus.xml\n","19_9ecb.xml\n","19_9ecbplus.xml\n","10_15ecbplus.xml\n","10_17ecbplus.xml\n","10_13ecbplus.xml\n","10_19ecbplus.xml\n","10_18ecbplus.xml\n","10_21ecbplus.xml\n","10_20ecbplus.xml\n","10_1ecbplus.xml\n","10_1ecb.xml\n","10_2ecb.xml\n","10_3ecbplus.xml\n","10_3ecb.xml\n","10_2ecbplus.xml\n","10_4ecb.xml\n","10_6ecbplus.xml\n","10_6ecb.xml\n","10_4ecbplus.xml\n","10_5ecb.xml\n","10_8ecb.xml\n","10_9ecbplus.xml\n","10_7ecb.xml\n","8_10ecbplus.xml\n","8_11ecbplus.xml\n","8_2ecb.xml\n","8_1ecb.xml\n","8_2ecbplus.xml\n","8_1ecbplus.xml\n","8_3ecb.xml\n","8_4ecb.xml\n","8_5ecb.xml\n","8_3ecbplus.xml\n","8_4ecbplus.xml\n","8_6ecb.xml\n","8_6ecbplus.xml\n","8_5ecbplus.xml\n","8_7ecb.xml\n","8_8ecb.xml\n","8_7ecbplus.xml\n","8_9ecbplus.xml\n","8_8ecbplus.xml\n","31_10ecbplus.xml\n","31_10ecb.xml\n","31_11ecb.xml\n","31_14ecb.xml\n","31_11ecbplus.xml\n","31_13ecb.xml\n","31_12ecb.xml\n","31_1ecb.xml\n","31_1ecbplus.xml\n","31_2ecb.xml\n","31_3ecbplus.xml\n","31_3ecb.xml\n","31_2ecbplus.xml\n","31_6ecb.xml\n","31_4ecb.xml\n","31_5ecbplus.xml\n","31_4ecbplus.xml\n","31_5ecb.xml\n","31_6ecbplus.xml\n","31_7ecbplus.xml\n","31_8ecb.xml\n","31_7ecb.xml\n","31_8ecbplus.xml\n","31_9ecbplus.xml\n","31_9ecb.xml\n","30_10ecb.xml\n","30_11ecb.xml\n","30_10ecbplus.xml\n","30_13ecb.xml\n","30_12ecbplus.xml\n","30_11ecbplus.xml\n","30_12ecb.xml\n","30_1ecbplus.xml\n","30_14ecb.xml\n","30_13ecbplus.xml\n","30_1ecb.xml\n","30_2ecbplus.xml\n","30_3ecbplus.xml\n","30_4ecb.xml\n","30_3ecb.xml\n","30_2ecb.xml\n","30_4ecbplus.xml\n","30_5ecb.xml\n","30_5ecbplus.xml\n","30_6ecb.xml\n","30_7ecb.xml\n","30_7ecbplus.xml\n","30_6ecbplus.xml\n","30_8ecb.xml\n","30_9ecbplus.xml\n","30_8ecbplus.xml\n","30_9ecb.xml\n","4_11ecb.xml\n","4_10ecbplus.xml\n","4_11ecbplus.xml\n","4_10ecb.xml\n","4_1ecbplus.xml\n","4_14ecb.xml\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4758/4758 [00:00<00:00, 8375.48it/s]"]},{"output_type":"stream","name":"stdout","text":["4_12ecb.xml\n","4_13ecb.xml\n","4_1ecb.xml\n","4_2ecb.xml\n","4_2ecbplus.xml\n","4_3ecb.xml\n","4_3ecbplus.xml\n","4_4ecbplus.xml\n","4_5ecb.xml\n","4_5ecbplus.xml\n","4_4ecb.xml\n","4_7ecb.xml\n","4_6ecbplus.xml\n","4_8ecbplus.xml\n","4_6ecb.xml\n","4_8ecb.xml\n","4_9ecb.xml\n","4_9ecbplus.xml\n","3_10ecbplus.xml\n","3_11ecbplus.xml\n","3_2ecbplus.xml\n","3_1ecbplus.xml\n","3_2ecb.xml\n","3_1ecb.xml\n","3_3ecb.xml\n","3_5ecb.xml\n","3_4ecb.xml\n","3_3ecbplus.xml\n","3_4ecbplus.xml\n","3_5ecbplus.xml\n","3_6ecb.xml\n","3_7ecbplus.xml\n","3_6ecbplus.xml\n","3_7ecb.xml\n","3_9ecb.xml\n","3_8ecb.xml\n","3_8ecbplus.xml\n","3_9ecbplus.xml\n","24_10ecb.xml\n","24_11ecb.xml\n","24_12ecb.xml\n","24_11ecbplus.xml\n","24_13ecb.xml\n","24_10ecbplus.xml\n","24_1ecb.xml\n","24_14ecb.xml\n","24_15ecb.xml\n","24_2ecb.xml\n","24_4ecb.xml\n","24_3ecb.xml\n","24_2ecbplus.xml\n","24_1ecbplus.xml\n","24_3ecbplus.xml\n","24_6ecb.xml\n","24_5ecb.xml\n","24_4ecbplus.xml\n","24_5ecbplus.xml\n","24_7ecb.xml\n","24_8ecb.xml\n","24_7ecbplus.xml\n","24_6ecbplus.xml\n","24_8ecbplus.xml\n","24_9ecbplus.xml\n","24_9ecb.xml\n","13_11ecb.xml\n","13_10ecbplus.xml\n","13_10ecb.xml\n","13_11ecbplus.xml\n","13_12ecbplus.xml\n","13_12ecb.xml\n","13_13ecb.xml\n","13_14ecbplus.xml\n","13_15ecb.xml\n","13_13ecbplus.xml\n","13_14ecb.xml\n","13_19ecb.xml\n","13_17ecb.xml\n","13_16ecb.xml\n","13_18ecb.xml\n","13_21ecb.xml\n","13_20ecb.xml\n","13_1ecb.xml\n","13_22ecb.xml\n","13_4ecb.xml\n","13_4ecbplus.xml\n","13_2ecb.xml\n","13_3ecb.xml\n","13_5ecb.xml\n","13_7ecb.xml\n","13_6ecbplus.xml\n","13_6ecb.xml\n","13_5ecbplus.xml\n","13_8ecbplus.xml\n","13_9ecb.xml\n","13_8ecb.xml\n","13_7ecbplus.xml\n","13_9ecbplus.xml\n","22_10ecbplus.xml\n","22_14ecbplus.xml\n","22_13ecbplus.xml\n","22_11ecbplus.xml\n","22_12ecbplus.xml\n","22_1ecb.xml\n","22_2ecbplus.xml\n","22_2ecb.xml\n","22_1ecbplus.xml\n","22_3ecb.xml\n","22_3ecbplus.xml\n","22_4ecbplus.xml\n","22_4ecb.xml\n","22_6ecb.xml\n","22_6ecbplus.xml\n","22_5ecbplus.xml\n","22_5ecb.xml\n","22_7ecb.xml\n","22_7ecbplus.xml\n","22_8ecb.xml\n","22_9ecbplus.xml\n","22_8ecbplus.xml\n","22_9ecb.xml\n","14_10ecb.xml\n","14_10ecbplus.xml\n","14_1ecb.xml\n","14_2ecb.xml\n","14_1ecbplus.xml\n","14_11ecbplus.xml\n","14_4ecb.xml\n","14_4ecbplus.xml\n","14_3ecb.xml\n","14_3ecbplus.xml\n","14_2ecbplus.xml\n","14_7ecb.xml\n","14_6ecbplus.xml\n","14_6ecb.xml\n","14_5ecbplus.xml\n","14_5ecb.xml\n","14_9ecb.xml\n","14_7ecbplus.xml\n","14_8ecbplus.xml\n","14_8ecb.xml\n","14_9ecbplus.xml\n","25_10ecb.xml\n","25_10ecbplus.xml\n","25_11ecbplus.xml\n","25_11ecb.xml\n","25_12ecb.xml\n","25_13ecb.xml\n","25_13ecbplus.xml\n","25_14ecbplus.xml\n","25_15ecb.xml\n","25_14ecb.xml\n","25_1ecb.xml\n","25_2ecb.xml\n","25_3ecb.xml\n","25_15ecbplus.xml\n","25_4ecb.xml\n","25_5ecbplus.xml\n","25_3ecbplus.xml\n","25_5ecb.xml\n","25_6ecb.xml\n","25_6ecbplus.xml\n","25_7ecb.xml\n","25_7ecbplus.xml\n","25_8ecb.xml\n","25_8ecbplus.xml\n","25_9ecb.xml\n","25_9ecbplus.xml\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["trainset = dict(zip(train_docs, train_info))\n","\n","file_path = ecb_path + \"processed/train_with_original_index.json\"\n","with open(file_path, 'w') as f:\n","    json.dump(trainset,f)"],"metadata":{"id":"mcV4wo6viKo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0-DMbNgbE3Y"},"source":[""],"execution_count":null,"outputs":[]}]}