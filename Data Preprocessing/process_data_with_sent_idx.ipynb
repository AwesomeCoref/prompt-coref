{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"process_data_with_sent_idx.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"1bc33c82cb1cc6717a2584fe196b3f3ec0300595df0b14d92dcede418ee0c5e5"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"cells":[{"cell_type":"code","metadata":{"id":"AOIIs4H46W5s","executionInfo":{"status":"ok","timestamp":1639360040915,"user_tz":-60,"elapsed":1093,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"source":["import os, re\n","from pprint import pprint\n","import spacy\n","from functools import partial\n","import pandas as pd\n","from tqdm import tqdm\n","import xml.etree.ElementTree as ET\n","import spacy\n","from spacy.lang.en import English\n","from spacy.tokenizer import Tokenizer\n","en = English()\n","tok_en = Tokenizer(en.vocab)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMnHQfn-baKk","executionInfo":{"status":"ok","timestamp":1639360041847,"user_tz":-60,"elapsed":937,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"source":["\n","# Load spacy since we need it for token tags and lemmas\n","nlp = spacy.load('en_core_web_sm', disable=['textcat'])\n","# dir_path = \"eda/data/ECB+/ECB+/\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYJVvDP8cZlq","executionInfo":{"status":"ok","timestamp":1639360088788,"user_tz":-60,"elapsed":16264,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"5cffef57-904c-421b-ca3c-481fd7be482d"},"source":["import random\n","from google.colab import drive\n","import pickle\n","drive.mount('/content/drive',force_remount=True)\n","\n","# Make sure to click \"Add shortcut to drive\" for the \"Coref-for-GPT\" folder\n","gdrive_dir_path = \"/content/drive/MyDrive/Coref-for-GPT\"\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["local_path = \"\"\n","\n","# Change this to \"local_path\" if you run the notebook locally\n","root_path = gdrive_dir_path"],"metadata":{"id":"IRSNpsegeRwR","executionInfo":{"status":"ok","timestamp":1639360097078,"user_tz":-60,"elapsed":302,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Path to the ecb data\n","ecb_path = f\"{root_path}/Data/ECB+/\"\n","ecb_input_path = f\"{root_path}/Data/ECB+/original/\""],"metadata":{"id":"wR5ygWf6eTdT","executionInfo":{"status":"ok","timestamp":1639360097320,"user_tz":-60,"elapsed":2,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNbZ1ynKbaKn","executionInfo":{"status":"ok","timestamp":1639360131794,"user_tz":-60,"elapsed":1754,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"source":["class ECBMentionsDoc:\n","\n","    # Problematic tokens in the dataset\n","    # From the CDLM repo\n","    error_tokens = [('31_10ecbplus.xml', 979),\n","                  ('9_3ecbplus.xml', 30),\n","                  ('9_4ecbplus.xml', 32)]\n","\n","\n","    def __init__(self, doc_path, doc_name, topic_id):\n","        self.doc_path = doc_path\n","        self.doc_name = doc_name\n","        self.topic_id = topic_id\n","        self.mentions_fields = {}\n","        self.mention_cluster_info = {}\n","        self.relation_source_target = {}\n","        self.relation_ids = {}\n","        self.relation_tag = {}\n","        self.event_singleton_idx = int(1E8)\n","        self.entity_singleton_idx = int(2E8)\n","        self.entity_mentions, self.event_mentions  = [], []\n","        self.clean_event_mentions, self.clean_entity_mentions = [], []\n","        self.tagged_event_tokens = {}\n","        self.tagged_entity_tokens = {}\n","        self.doc_token_texts = {}\n","        self.b_open, self.b_close = \"{\", \"}\"\n","        self.prev_wrap, self.prev_tag_id = '', ''\n","        self.tag_is_opened = False\n","        self.keep_start_url = True\n","        self.plain_tokens, self.clean_tokens = [], []\n","        self.plain_text, self.clean_text = '', ''\n","        self.sents_divider_idx = []\n","\n","\n","    def parse_xml(self):\n","        # Start parsing\n","        self.root = ET.parse(self.doc_path).getroot()\n","\n","        # Set all mention ids from the full document for both event and entity mentions\n","        self.set_all_marked_mentions()\n","\n","        # Set all cross doc ids\n","        self.set_cross_doc_mentions()\n","\n","        # Creates both arrays containing all event and entity mention info\n","        self.compute_event_entity_mentions()\n","\n","        # Parses all the actual tokens from the current document into a dict we can use\n","        self.set_doc_texts()\n","\n","\n","    # Loops through each word in the full document and stores the info in a dict like this:\n","    def set_doc_texts(self):\n","        '''\n","        Example text:\n","        <token t_id=\"53\" sentence=\"4\" number=\"1\">Williams</token>\n","        <token t_id=\"54\" sentence=\"4\" number=\"2\">,</token>\n","        <token t_id=\"55\" sentence=\"4\" number=\"3\">the</token>\n","        <token t_id=\"56\" sentence=\"4\" number=\"4\">swimming</token>\n","        <token t_id=\"57\" sentence=\"4\" number=\"5\">champion</token>\n","        <token t_id=\"58\" sentence=\"4\" number=\"6\">turned</token>\n","        <token t_id=\"59\" sentence=\"4\" number=\"7\">actress</token>\n","        '''\n","        prev_sent_id = -1\n","        for token in self.root.findall('token'):\n","            token_id = int(token.get('t_id'))\n","            \n","            # A few tokens per should not be used\n","            if (self.doc_name, token_id) not in self.error_tokens:\n","                # Parse actual token text in the right format\n","                token_text = token.text.replace('ï¿½', '').strip()\n","                sent_id = int(token.get('sentence'))\n","\n","                # word_id_sent = token.get('number') # word index per sentence\n","                token_info = (token_text, sent_id)\n","                \n","                # Write data for sentence reconstruction\n","                if (prev_sent_id > -1) and (sent_id != prev_sent_id):\n","                    prev_token_info = self.doc_token_texts[prev_token_id]\n","                    self.doc_token_texts[prev_token_id] = (prev_token_info[0] + \" [EOS]\", prev_token_info[1])\n","\n","                self.doc_token_texts[token_id] = token_info\n","                prev_sent_id = sent_id\n","                prev_token_id = token_id\n","\n","\n","    # Maps each mentions to a possible relation is has, meaning singleton or not\n","    # Then saves the info from the self.mentions_fields for each mention with\n","    # additional info like if it's cluster or not and the description of the cluster\n","    # It also splits them into 2 buckets: Event and Entity mentions\n","    def compute_event_entity_mentions(self):\n","        \n","        # Loop through all mentions of the current document\n","        for m_id, mention in self.mentions_fields.items():\n","\n","            # For this specific mention check if's a source by checking if it maps to a target\n","            # Since the dict containts {source_mention_id: target_mention_id}\n","            target_id = self.relation_source_target.get(m_id, None)\n","\n","            # If it's just a source_id with no second target_id in it's cluster;\n","            # then we know that this event or enntity mention has to be a singleton\n","            if target_id is None:\n","                if mention['event']:\n","                    cluster_id = self.event_singleton_idx\n","                    self.event_singleton_idx += 1\n","                else:\n","                    cluster_id = self.entity_singleton_idx\n","                    self.entity_singleton_idx += 1\n","\n","                # cluster_id =  'Singleton_' + file_name + '_' + m_id\n","                cluster_desc = ''\n","            else:\n","                # Relation id is basically the cluster's id to identify a cluser\n","                r_id = self.relation_ids[target_id]\n","                tag = self.relation_tag[target_id] # E.g. CROSS_DOC_COREF\n","                \n","                # Only within doc link\n","                if tag.startswith('INTRA'):\n","                    # Entity and event mentions may have the same intra cluster id \n","                    suffix = '1' if mention['event'] else '0' \n","                    cluster_id =  int(r_id + suffix)\n","                else:\n","                    # Grab the cluster info dict from the mention clusters we created\n","                    target_cluster_info = self.mention_cluster_info[target_id]\n","\n","                    # E.g. ACT16236402809085484\n","                    target_cluster_id_str = target_cluster_info['cluster_id']\n","\n","                    # We grab all the integers from this string to construct an int we can use\n","                    cluster_id = int(target_cluster_id_str[3:])\n","\n","                # e.g. t4_swimming_skills\n","                cluster_desc = self.mention_cluster_info[target_id]['cluster_desc']\n","\n","\n","            # Now that we retrieved the cluster id and description for this mention;\n","            # We can update the mention dict we create before with this and append;\n","            # it to the entities correpsonding group -> Event or Entity mention\n","            mention_info = mention.copy()\n","            mention_info[\"cluster_id\"] = cluster_id\n","            mention_info[\"cluster_desc\"] = cluster_desc\n","            event = mention_info.pop(\"event\")\n","            if not event:\n","                self.entity_mentions.append(mention_info)\n","            else:\n","                self.event_mentions.append(mention_info)\n","\n","\n","    # Set a dict with cross doc relations, meaning the mention id of the target for this doc\n","    # with the relation id, which is the crossdoc id, it's saved like this:\n","    # {target_mention_id: relation_id}\n","    def set_cross_doc_mentions(self):\n","        '''\n","        Example part to parse:\n","        <CROSS_DOC_COREF r_id=\"22306\" note=\"ACT16195873839112917\">\n","            <source m_id=\"28\" />\n","            <source m_id=\"34\" />\n","            <target m_id=\"60\" />\n","        </CROSS_DOC_COREF>\n","        '''\n","\n","        # Relation -> Cross doc relation\n","        for relation in self.root.find('Relations'):\n","            \n","            # Last element of each cluster is 'target'\n","            target_mention_id = relation[-1].attrib['m_id']\n","            \n","            # All the other elements are of type 'source'\n","            source_tags = relation[:-1]\n","\n","            # Set a mapping from coref source id to it's master target\n","            for source_tag in source_tags:\n","                source_mention_id = source_tag.attrib['m_id']\n","                self.relation_source_target[source_mention_id] = target_mention_id\n","\n","            \n","            # Save tag 'CROSS_DOC_COREF' \n","            self.relation_tag[target_mention_id] = relation.tag\n","\n","            # Save the target mention id to cross doc id entries\n","            self.relation_ids[target_mention_id] = relation.attrib['r_id']\n","\n","\n","    def set_all_marked_mentions(self):\n","        '''\n","        Example part to parse:\n","        <ACTION_ASPECTUAL m_id=\"53\">\n","            <token_anchor t_id=\"186\"/>\n","        </ACTION_ASPECTUAL>\n","        <ACTION_OCCURRENCE m_id=\"50\">\n","            <token_anchor t_id=\"179\"/>\n","            <token_anchor t_id=\"180\"/>\n","            <token_anchor t_id=\"181\"/>\n","        </ACTION_OCCURRENCE>\n","        '''\n","\n","        # Store our results\n","        subtopic = '0' if 'plus' in self.doc_name else '1'\n","\n","        for mention in self.root.find('Markables'):\n","            m_id = mention.attrib['m_id']\n","\n","            if 'RELATED_TO' not in mention.attrib:\n","\n","                # ACTION or NEG is an event mention \n","                is_event_mention = mention.tag.startswith('ACT') or mention.tag.startswith('NEG')\n","                \n","                # Grab all token ids under current Markable tag\n","                tokens_ids = [int(term.attrib['t_id']) for term in mention]\n","\n","                if len(tokens_ids) == 0:\n","                    print(ET.tostring(mention, encoding='unicode'))\n","                    continue\n","\n","                # print(is_event_mention, tokens_ids)\n","\n","                # Indexing our sentences also starts at 0\n","                token_sent_index = tokens_ids[0]\n","                sent_id = self.root[token_sent_index].attrib['sentence']\n","\n","                # Construct the actual mention text, e.g. \"Barack Obama\"\n","                # NOTE: We -1 the token id itself, since they started indexing at 1 and map starts at 0\n","                mention_word_tokens = ' '.join(list(map(lambda x: self.root[x-1].text, tokens_ids)))\n","\n","                lemmas, tags = [], []\n","                for tok in nlp(mention_word_tokens):\n","                    lemmas.append(tok.lemma_)\n","                    tags.append(tok.tag_)\n","                \n","                self.mentions_fields[m_id] = {\n","                    \"doc_id\": self.doc_name,\n","                    \"topic\": self.topic_id,\n","                    \"subtopic\": self.doc_name.split('_')[0] + '_' + subtopic,\n","                    \"m_id\": m_id,\n","                    \"sentence_id\" : int(sent_id),\n","                    \"tokens_ids\": tokens_ids,\n","                    \"tokens\": mention_word_tokens,\n","                    \"tags\": ' '.join(tags),\n","                    \"lemmas\": ' '.join(lemmas),\n","                    \"event\": is_event_mention\n","                }\n","            else:\n","                self.mention_cluster_info[m_id] = {\n","                    \"cluster_id\": mention.attrib.get('instance_id', ''),\n","                    \"cluster_desc\": mention.attrib['TAG_DESCRIPTOR']\n","                }\n","                   \n","                \n","\n","    def clear_url(self):\n","        \n","        no_space_doc =  self.original_text.replace(\" \", \"\")\n","\n","        print(no_space_doc)\n","\n","\n","    # http://www.ws.com/May 2, 2013.. -> http://www.ws.com/ May 2, 2013.. or May 2, 2013..\n","    def split_url_on_month(self, match):\n","        matched_url = match.group()\n","        months = ['Lindsay', 'Former', 'Footage', 'Video', '.html', 'Gunman',\n","                  'Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n","        # print(matched_url)\n","        # Minimum amount of characters for an url plus month string\n","        if len(matched_url) > 10 and any([x in matched_url for x in months]):\n","            \n","            # Compile a regex with each month as option\n","            regexPattern = '|'.join(map(re.escape, months))\n","            \n","            # Split the string by 1 of the months, also keeping the matched month itself\n","            matches = re.split(f\"({regexPattern})\", matched_url, 1)\n","            new_url, month = matches[0], matches[1]   \n","            \n","            # So we want to keep the original url            \n","            if self.keep_start_url :\n","            \n","                # Return the url with the space in between the month\n","                return f\"{new_url} {month}\"\n","            \n","            return month\n","\n","        \n","        # So we want to keep the original url            \n","        if self.keep_start_url :\n","            return matched_url\n","        \n","        # We can just skip the url altogether\n","        return ''            \n","    \n","\n","    # https://stackoverflow.com/questions/21948019/python-untokenize-a-sentence\n","    # https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py\n","    def create_clean_text(self, text):\n","        \"\"\"\n","        Untokenizing a text undoes the tokenizing operation, restoring\n","        punctuation and spaces to the places that people expect them to be.\n","        Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","        except for line breaks.\n","        \"\"\"\n","        text = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","        text = text.replace(\" ( \", \" (\").replace(\" ) \", \") \").replace(\" 's\", \"'s\")\n","        text = re.sub(r' ([.,:;?!%]+)([ \\'`])', r\"\\1\\2\", text)\n","        text = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", text)\n","        text = text.replace(\" n't\", \"n't\").replace(\"can not\", \"cannot\")\n","        text = text.replace(\" ` \", \" '\").replace(\" -\", \"-\").replace(\"- \", \"-\")\n","        text = text.replace(\" ,\", \",\").replace(' /',  '/').replace('/ ',  '/')\n","        text = text.replace(\" â s\", \"'s\").replace(\"â \", \"â\").replace(\" â\", \"â\")\n","        text = text.replace(\" â s\", \"'s\").replace(\"â \", \"â\").replace(\" â\", \"â\")\n","        text = text.replace(\"www. \", \"www.\").replace(\". com\", \".com\").replace(\" â\", \"â\")\n","        text = text.replace(\" _ \", \"_\")\n","        text = text.replace(\"p. m.\", \"p.m.\").replace(\"a. m.\", \"a.m.\")\n","        text = text.replace(\"P. M.\", \"P.M.\").replace(\"A. M.\", \"A.M.\")\n","        text = text.replace(\"[EOS]\", \"\")\n","\n","        # Regex to match even amount of \", because removing trailing or start space;\n","        # Will also remove any characters before and after quotes start.\n","        # So we need to match the even amount, see: https://stackoverflow.com/a/53436792/8970591\n","        # Inspiration for regex: https://stackoverflow.com/questions/14906492/how-can-whitespace-be-trimmed-from-a-regex-capture-group\n","        quote_regex = '\\\\\"\\s?([^\\]]*?)\\s?\\\\\"'\n","        text = re.sub(quote_regex, '\\\"'+r'\\1'+'\\\"' , text)\n","        \n","        # Re-replace this exception\n","        text = text.replace(',\"', ', \"').replace('.\"', '. \"')\n","\n","        # A lot of articles start with an url in the as the source it came from\n","        # So we can optionally get rid of this to get a cleaner text for text generation\n","        first_url_regex = '^(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'        \n","        \n","        # Also stops match if there is a Capital letter after the last .com, .nl etc\n","        # first_url_regex = '^(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([a-z0-9_.,@?^=%&:\\/~+#-]*[a-z0-9_@?^=%&\\/~+#-])'\n","        pattern = re.compile(first_url_regex)\n","        \n","        # Partials can be used to make new derived functions that have some input parameters pre-assigned\n","        re_sub_callback = partial(self.split_url_on_month)\n","        \n","        # Sometime the first word, which is a month, can be captured by our regex\n","        # So we want to split on this character to we can keep the month\n","        text = re.sub(pattern, re_sub_callback, text, 1)\n","        \n","        tokenize_text = [i.text for i in nlp(text)] \n","        \n","        return text, tokenize_text\n","\n","\n","    def format_doc_and_mentions(self):\n","        self.plain_tokens = [el[1][0] for el in self.doc_token_texts.items()] \n","        self.plain_text = ' '.join(word for word in self.plain_tokens)\n","        sents = self.plain_text.split(\"[EOS]\")\n","        for sent in sents:\n","            # print(sent)\n","            clean_text, clean_tokens = self.create_clean_text(sent)\n","            self.clean_text += clean_text + \"[EOS]\"\n","            self.clean_tokens += clean_tokens \n","            self.sents_divider_idx.append(len(self.clean_tokens))\n","        \n","    \n","    def clean_and_reindex(self): \n","        # Creates a clean version of the text along with the raw one\n","        self.format_doc_and_mentions()\n","        self.reindex_all_marked_mentions()\n","\n","    # You can pass a key function to sorted which\n","    # returns a tuple containing the two things you wish to sort on\n","    def sort_by_token_ids(self, mention_item):\n","        first_token_index = mention_item['tokens_ids'][0]\n","        return first_token_index\n","\n","\n","    def get_span_by_ids(self, span_token_ids):\n","        span_tokens = []\n","        \n","        for token_id, (token, _) in self.doc_token_texts.items():\n","            if token_id in span_token_ids:\n","                span_tokens.append(token)\n","                \n","        return span_tokens\n","\n","    def reindex_all_marked_mentions(self):\n","        '''\n","        Input -> {m_id: all_mention_info}\n","        {'1': \n","            {'doc_id': '1_1ecbplus.xml',\n","            'event': False,\n","            'lemmas': 'June 13 , 2013 4 : 59 PM EDT',\n","            'm_id': '1',\n","            'sentence_id': 2,\n","            'subtopic': '1_0',\n","            'tags': 'NNP CD , CD CD SYM CD NNP NNP',\n","            'tokens': 'June 13 , 2013 4 : 59 PM EDT',\n","            'tokens_ids': [45, 46, 47, 48, 49, 50, 51, 52, 53],\n","            'topic': 2},\n","        }\n","        '''\n","        # Sorts the mention info dict by the first token_id of each mention\n","        # So the mentions are in sequential order\n","        entity_mention_info_sorted = sorted(self.entity_mentions, key = self.sort_by_token_ids)\n","        event_mention_info_sorted = sorted(self.event_mentions, key = self.sort_by_token_ids)\n","\n","        # pprint(event_mention_info_sorted)\n","        # New dictionaries to use for updated spans and indices (token_ids) of them \n","        self.clean_entity_mentions = self.reindex_mentions(entity_mention_info_sorted)\n","        # self.clean_event_mentions = self.reindex_mentions(event_mention_info_sorted)\n","        \n","    def get_new_token_index(self, token, start_index):\n","        if token in self.clean_tokens[start_index:]:\n","            new_token_index = self.clean_tokens.index(token, start_index) # Changed\n","        else:\n","            # if token == \"m\":\n","            #     return \n","            # Could happen that our original mention has made a different split then the next text\n","            # E.g. 'facility in Malibu, Calif' -> 'facility in Malibu, Calif.'\n","            new_token_index = -1\n","            for i, tok_in_sent in enumerate(self.clean_tokens[start_index:]):\n","                if tok_in_sent[:len(token)] == token:\n","                    new_token_index = i+start_index\n","                    break\n","            if new_token_index == -1:\n","                raise Exception(f\"ERROR ({self.doc_path}) : after index {start_index}, '{token}' is not part of -> '{self.clean_tokens}' \")\n","        return new_token_index\n","\n","\n","\n","    def reindex_mentions(self, mention_info_sorted):\n","                \n","        new_mentions = {}\n","        start_index = 0\n","        \n","        # pprint(self.mentions_fields)            \n","        for m_id, mention_info in enumerate(mention_info_sorted):\n","            # raw_mention_str = mention_info['tokens']\n","            raw_token_ids = mention_info['tokens_ids']\n","            raw_mention_tokens = self.get_span_by_ids(raw_token_ids)\n","            \n","            if m_id > 0:\n","                prev_m_id = m_id-1\n","                prev_m = mention_info_sorted[prev_m_id]\n","                prev_raw_token_ids = prev_m['tokens_ids']\n","                # Deal with overlapping mentions, ex., m1=[5,6,7,8], m2=[6,7] \n","                if prev_raw_token_ids[-1] >= raw_token_ids[-1]:\n","                    start_index = new_mentions[prev_m_id]['tokens_ids'][0]\n","                else:\n","                    start_index = new_mentions[prev_m_id]['tokens_ids'][-1]\n","\n","            # Clean and create tokens from raw mention text, \n","            # so we can use that to match all the current cleaned tokens\n","            mention_token_srt, mention_tokens = self.create_clean_text(\" \".join(raw_mention_tokens))\n","            # Get new tokens and new token ids from the current mention\n","            new_tokens_ids = []\n","            new_mention_tokens = []\n","            \n","            for token in mention_tokens:\n","                new_token_index = self.get_new_token_index(token, start_index+1)                   \n","                new_tokens_ids.append(new_token_index)\n","                new_mention_tokens.append(self.clean_tokens[new_token_index])\n","                start_index = new_token_index\n","            \n","            # Create new update dict with new values\n","            clean_mention_info = mention_info\n","            clean_mention_info['tokens'] = new_mention_tokens\n","            clean_mention_info['tokens_ids'] = new_tokens_ids\n","            \n","            new_mentions[m_id] = clean_mention_info\n","            # print(f\"mention {m_id}: {new_tokens_ids};  {raw_token_ids}; {new_mention_tokens}, {raw_mention_tokens}\", )         \n","        return new_mentions\n","            \n","    def get_clusters(self, mentions):\n","        clusters = {}\n","        for m_id in mentions:\n","            mention = mentions[m_id]\n","            cluster_id = mention['cluster_id']\n","            # Create empty list entry if not existent\n","            if cluster_id not in clusters:\n","                clusters[cluster_id] = []\n","            clusters[cluster_id] += mention[\"tokens_ids\"]\n","\n","        return clusters\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Running the doc-level parser\n","We run the doc-level parser on 1 document to inspect it's outputs and if it's running correctly"],"metadata":{"id":"2GwPCH7LejCJ"}},{"cell_type":"code","source":["# Test cell\n","file_name = \"1_1ecb.xml\"\n","topic = file_name.split(\"_\")[0]\n","ecb_doc_path = f\"{ecb_input_path}/{1}/{file_name}\"\n","ecb_mention_doc = ECBMentionsDoc(ecb_doc_path, file_name, 2)\n","ecb_mention_doc.parse_xml()\n","ecb_mention_doc.clean_and_reindex()\n","\n","print(\"Original untokenized text\")\n","print(ecb_mention_doc.plain_text)\n","print(\"\\n\\n Cleaned untokenized text\")\n","print(ecb_mention_doc.clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_gC_QZ6ej1y","executionInfo":{"status":"ok","timestamp":1639360134102,"user_tz":-60,"elapsed":2311,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"66aeb61f-b675-4672-8d04-d649f83bf269"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original untokenized text\n","Another day in Hollywood ; another star in rehab . [EOS] Word comes from People magazine and other celebrity news outlets that Tara Reid , 33 , who starred in `` American Pie '' and appeared on U.S. TV show `` Scrubs , ' ' has entered the Promises Treatment Center in Malibu , California - the same facility that in the past has been the rehab facility of choice for many a Hollywood star . [EOS] People said Reid 's representative Jack Ketsoyan confirmed the actress 's stay at Promises .\n","\n","\n"," Cleaned untokenized text\n","Another day in Hollywood; another star in rehab. [EOS] Word comes from People magazine and other celebrity news outlets that Tara Reid, 33, who starred in \"American Pie\" and appeared on U.S. TV show \"Scrubs, ' ' has entered the Promises Treatment Center in Malibu, California-the same facility that in the past has been the rehab facility of choice for many a Hollywood star. [EOS] People said Reid's representative Jack Ketsoyan confirmed the actress's stay at Promises.[EOS]\n"]}]},{"cell_type":"code","metadata":{"id":"JjpsBntaGDCM","executionInfo":{"status":"ok","timestamp":1639360140664,"user_tz":-60,"elapsed":332,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"source":["class ECBMentionsDataset:\n","\n","    def __init__(self, path, topic_ids):\n","        self.path = path\n","        self.selected_topic_ids = topic_ids\n","        self.docs = {}\n","\n","    def parse_data(self):\n","        if not os.path.exists(self.path):\n","        # if not Path(self.path).exists():\n","            raise f\"{self.path} does not exists!\"\n","\n","        topic_count = 0\n","        # Gets a list in format of [(topic_id, path_to_topic_dir)]\n","        for topic_dir in os.scandir(self.path):\n","\n","            # Validate of content of ECB dirs actually are directories n mumeric form e.g. '1'\n","            if topic_dir.is_dir() and topic_dir.name.isnumeric():\n","                topic_id = int(topic_dir.name)\n","\n","                if topic_id in self.selected_topic_ids:\n","                    topic_count += 1\n","                    self.parse_topic_docs(topic_dir.path, topic_id)\n","                    perc = (topic_count/(len(self.selected_topic_ids ))) * 100\n","                    print(f\"{round(perc, 2)}% Done parsing topic -> \\t {topic_id}\")\n","                \n","            else:\n","                print(f\"Skipping dir/file '{topic_dir.name}' in topic parsing because not a directory or number like...\")\n","\n","\n","\n","    def parse_topic_docs(self, topic_path, topic_id):\n","        # Loop through all the files of the specific topic dir\n","        for ecb_file in os.scandir(topic_path):\n","            if ecb_file.is_file() and ecb_file.name: #not in self.skip_list:\n","                print(f\"Parsing -> {ecb_file.name}\")\n","                # Creates in instance of the class that handles all document;\n","                # level parsing for: Actual doc's word + event and entity mentions\n","                try:\n","                    ecb_doc = ECBMentionsDoc(ecb_file.path, ecb_file.name, topic_id)\n","                    ecb_doc.parse_xml()\n","                    ecb_doc.clean_and_reindex()\n","\n","                    clean_text = ecb_doc.clean_text\n","                    clean_tokens = ecb_doc.clean_tokens\n","                    entity_mentions = ecb_doc.clean_entity_mentions\n","                    clusters = ecb_doc.get_clusters(entity_mentions)\n","                    sents_divider_idx = ecb_doc.sents_divider_idx\n","\n","                    # Extend this corpus' data from the current document level information\n","                    self.docs[ecb_file.name] = [clean_text, clean_tokens, entity_mentions, clusters, sents_divider_idx]\n","                except:\n","                    print(f\"Skip {ecb_file.name}\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxuJ9U4abaLC","executionInfo":{"status":"ok","timestamp":1639360157416,"user_tz":-60,"elapsed":282,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}}},"source":["VALIDATION = [2, 5, 12, 18, 21, 23, 34, 35]\n","TRAIN = [i for i in range(1, 36) if i not in VALIDATION]\n","TEST = [i for i in range(36, 46)]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Pp3k2eGbaOX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639360306742,"user_tz":-60,"elapsed":147738,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"b6c10f22-1ab6-416e-adea-59bed1758d86"},"source":["dev = ECBMentionsDataset(ecb_input_path, VALIDATION)\n","dev.parse_data()\n","dev_data = dev.docs"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing -> 35_10ecb.xml\n","Parsing -> 35_10ecbplus.xml\n","Parsing -> 35_1ecbplus.xml\n","Parsing -> 35_11ecbplus.xml\n","Parsing -> 35_1ecb.xml\n","Parsing -> 35_3ecbplus.xml\n","Parsing -> 35_2ecbplus.xml\n","Skip 35_2ecbplus.xml\n","Parsing -> 35_2ecb.xml\n","Skip 35_2ecb.xml\n","Parsing -> 35_3ecb.xml\n","Parsing -> 35_5ecbplus.xml\n","Parsing -> 35_4ecbplus.xml\n","Parsing -> 35_5ecb.xml\n","Parsing -> 35_4ecb.xml\n","Parsing -> 35_7ecbplus.xml\n","Parsing -> 35_6ecb.xml\n","Parsing -> 35_6ecbplus.xml\n","Parsing -> 35_7ecb.xml\n","Parsing -> 35_8ecb.xml\n","Parsing -> 35_9ecb.xml\n","Parsing -> 35_9ecbplus.xml\n","Parsing -> 35_8ecbplus.xml\n","12.5% Done parsing topic -> \t 35\n","Parsing -> 34_10ecbplus.xml\n","Parsing -> 34_11ecb.xml\n","Skip 34_11ecb.xml\n","Parsing -> 34_10ecb.xml\n","Skip 34_10ecb.xml\n","Parsing -> 34_12ecbplus.xml\n","Parsing -> 34_13ecb.xml\n","Skip 34_13ecb.xml\n","Parsing -> 34_12ecb.xml\n","Skip 34_12ecb.xml\n","Parsing -> 34_11ecbplus.xml\n","Parsing -> 34_16ecb.xml\n","Parsing -> 34_1ecb.xml\n","Parsing -> 34_14ecb.xml\n","Parsing -> 34_15ecb.xml\n","Skip 34_15ecb.xml\n","Parsing -> 34_2ecb.xml\n","Parsing -> 34_1ecbplus.xml\n","Parsing -> 34_2ecbplus.xml\n","Parsing -> 34_3ecb.xml\n","Parsing -> 34_4ecbplus.xml\n","Parsing -> 34_5ecbplus.xml\n","Parsing -> 34_3ecbplus.xml\n","Parsing -> 34_4ecb.xml\n","Parsing -> 34_5ecb.xml\n","Parsing -> 34_6ecb.xml\n","Skip 34_6ecb.xml\n","Parsing -> 34_7ecb.xml\n","Parsing -> 34_6ecbplus.xml\n","Parsing -> 34_8ecbplus.xml\n","Parsing -> 34_9ecb.xml\n","Parsing -> 34_7ecbplus.xml\n","Parsing -> 34_8ecb.xml\n","Parsing -> 34_9ecbplus.xml\n","Skip 34_9ecbplus.xml\n","25.0% Done parsing topic -> \t 34\n","Parsing -> 18_10ecbplus.xml\n","Parsing -> 18_11ecb.xml\n","Parsing -> 18_10ecb.xml\n","Parsing -> 18_11ecbplus.xml\n","Parsing -> 18_12ecb.xml\n","Skip 18_12ecb.xml\n","Parsing -> 18_13ecb.xml\n","Parsing -> 18_1ecb.xml\n","Parsing -> 18_15ecb.xml\n","Parsing -> 18_1ecbplus.xml\n","Parsing -> 18_16ecb.xml\n","Parsing -> 18_14ecb.xml\n","Parsing -> 18_2ecbplus.xml\n","Parsing -> 18_2ecb.xml\n","Parsing -> 18_3ecb.xml\n","Skip 18_3ecb.xml\n","Parsing -> 18_3ecbplus.xml\n","Skip 18_3ecbplus.xml\n","Parsing -> 18_4ecb.xml\n","Parsing -> 18_5ecbplus.xml\n","Parsing -> 18_4ecbplus.xml\n","Parsing -> 18_5ecb.xml\n","Parsing -> 18_6ecbplus.xml\n","Parsing -> 18_8ecb.xml\n","Parsing -> 18_7ecb.xml\n","Parsing -> 18_6ecb.xml\n","Parsing -> 18_7ecbplus.xml\n","Parsing -> 18_9ecbplus.xml\n","Parsing -> 18_8ecbplus.xml\n","Skip 18_8ecbplus.xml\n","Parsing -> 18_9ecb.xml\n","37.5% Done parsing topic -> \t 18\n","Parsing -> 21_10ecbplus.xml\n","Parsing -> 21_10ecb.xml\n","Parsing -> 21_12ecbplus.xml\n","Parsing -> 21_11ecb.xml\n","Parsing -> 21_11ecbplus.xml\n","Parsing -> 21_13ecbplus.xml\n","Parsing -> 21_12ecb.xml\n","Parsing -> 21_1ecbplus.xml\n","Parsing -> 21_1ecb.xml\n","Parsing -> 21_14ecbplus.xml\n","Parsing -> 21_2ecb.xml\n","Parsing -> 21_2ecbplus.xml\n","Parsing -> 21_3ecb.xml\n","Parsing -> 21_3ecbplus.xml\n","Skip 21_3ecbplus.xml\n","Parsing -> 21_4ecb.xml\n","Parsing -> 21_4ecbplus.xml\n","Parsing -> 21_6ecbplus.xml\n","Parsing -> 21_5ecbplus.xml\n","Parsing -> 21_5ecb.xml\n","Parsing -> 21_6ecb.xml\n","Parsing -> 21_7ecbplus.xml\n","Parsing -> 21_7ecb.xml\n","Parsing -> 21_9ecb.xml\n","Parsing -> 21_8ecbplus.xml\n","Parsing -> 21_8ecb.xml\n","Skip 21_8ecb.xml\n","Parsing -> 21_9ecbplus.xml\n","50.0% Done parsing topic -> \t 21\n","Parsing -> 23_10ecb.xml\n","Skip 23_10ecb.xml\n","Parsing -> 23_10ecbplus.xml\n","Parsing -> 23_11ecbplus.xml\n","Parsing -> 23_1ecb.xml\n","Parsing -> 23_2ecb.xml\n","Parsing -> 23_1ecbplus.xml\n","Parsing -> 23_3ecb.xml\n","Parsing -> 23_4ecb.xml\n","Skip 23_4ecb.xml\n","Parsing -> 23_3ecbplus.xml\n","Parsing -> 23_2ecbplus.xml\n","Parsing -> 23_6ecb.xml\n","Skip 23_6ecb.xml\n","Parsing -> 23_5ecbplus.xml\n","Parsing -> 23_4ecbplus.xml\n","Parsing -> 23_5ecb.xml\n","Parsing -> 23_6ecbplus.xml\n","Parsing -> 23_8ecb.xml\n","Parsing -> 23_7ecbplus.xml\n","Parsing -> 23_7ecb.xml\n","Parsing -> 23_8ecbplus.xml\n","Parsing -> 23_9ecb.xml\n","Parsing -> 23_9ecbplus.xml\n","62.5% Done parsing topic -> \t 23\n","Parsing -> 12_10ecb.xml\n","Parsing -> 12_11ecb.xml\n","Parsing -> 12_11ecbplus.xml\n","Parsing -> 12_12ecb.xml\n","Parsing -> 12_10ecbplus.xml\n","Parsing -> 12_13ecb.xml\n","Parsing -> 12_15ecb.xml\n","Skip 12_15ecb.xml\n","Parsing -> 12_16ecb.xml\n","Skip 12_16ecb.xml\n","Parsing -> 12_17ecb.xml\n","Parsing -> 12_14ecb.xml\n","Parsing -> 12_1ecbplus.xml\n","Parsing -> 12_2ecb.xml\n","Parsing -> 12_18ecb.xml\n","Parsing -> 12_1ecb.xml\n","Parsing -> 12_19ecb.xml\n","Parsing -> 12_3ecbplus.xml\n","Parsing -> 12_2ecbplus.xml\n","Parsing -> 12_4ecb.xml\n","Parsing -> 12_3ecb.xml\n","Parsing -> 12_5ecbplus.xml\n","Parsing -> 12_5ecb.xml\n","Parsing -> 12_4ecbplus.xml\n","Parsing -> 12_6ecb.xml\n","Parsing -> 12_8ecb.xml\n","Parsing -> 12_6ecbplus.xml\n","Parsing -> 12_7ecb.xml\n","Parsing -> 12_8ecbplus.xml\n","Parsing -> 12_7ecbplus.xml\n","Parsing -> 12_9ecb.xml\n","Parsing -> 12_9ecbplus.xml\n","75.0% Done parsing topic -> \t 12\n","Parsing -> 5_10ecb.xml\n","Skip 5_10ecb.xml\n","Parsing -> 5_12ecb.xml\n","Parsing -> 5_10ecbplus.xml\n","Parsing -> 5_13ecb.xml\n","Parsing -> 5_11ecb.xml\n","Skip 5_11ecb.xml\n","Parsing -> 5_2ecb.xml\n","Skip 5_2ecb.xml\n","Parsing -> 5_14ecb.xml\n","Skip 5_14ecb.xml\n","Parsing -> 5_1ecb.xml\n","Skip 5_1ecb.xml\n","Parsing -> 5_4ecb.xml\n","Parsing -> 5_2ecbplus.xml\n","Parsing -> 5_3ecbplus.xml\n","Parsing -> 5_3ecb.xml\n","Parsing -> 5_6ecb.xml\n","Parsing -> 5_5ecb.xml\n","Skip 5_5ecb.xml\n","Parsing -> 5_4ecbplus.xml\n","Parsing -> 5_5ecbplus.xml\n","Parsing -> 5_6ecbplus.xml\n","Parsing -> 5_7ecbplus.xml\n","Parsing -> 5_9ecbplus.xml\n","Parsing -> 5_8ecbplus.xml\n","Parsing -> 5_7ecb.xml\n","Parsing -> 5_9ecb.xml\n","Skip 5_9ecb.xml\n","Parsing -> 5_1ecbplus.xml\n","87.5% Done parsing topic -> \t 5\n","Parsing -> 2_1ecb.xml\n","Parsing -> 2_11ecb.xml\n","Parsing -> 2_10ecbplus.xml\n","Skip 2_10ecbplus.xml\n","Parsing -> 2_11ecbplus.xml\n","Parsing -> 2_3ecbplus.xml\n","Parsing -> 2_2ecb.xml\n","Parsing -> 2_2ecbplus.xml\n","Parsing -> 2_3ecb.xml\n","Parsing -> 2_1ecbplus.xml\n","Parsing -> 2_4ecbplus.xml\n","Parsing -> 2_5ecb.xml\n","Parsing -> 2_5ecbplus.xml\n","Parsing -> 2_4ecb.xml\n","Parsing -> 2_6ecb.xml\n","Parsing -> 2_6ecbplus.xml\n","Parsing -> 2_7ecb.xml\n","Parsing -> 2_7ecbplus.xml\n","Parsing -> 2_9ecb.xml\n","Parsing -> 2_9ecbplus.xml\n","Parsing -> 2_8ecb.xml\n","Parsing -> 2_8ecbplus.xml\n","100.0% Done parsing topic -> \t 2\n"]}]},{"cell_type":"code","source":["print(len(dev_data))\n","\n","file_path = ecb_path + \"processed/dev_with_sent_idx.json\"\n","with open(file_path, 'w') as f:\n","    json.dump(dev_data, f)"],"metadata":{"id":"Q0L_eVlgdH0D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639360308843,"user_tz":-60,"elapsed":231,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"cc42efdd-088f-4b08-a748-3cba57d82679"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["169\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whprXn9JbaQQ","executionInfo":{"status":"ok","timestamp":1639360774977,"user_tz":-60,"elapsed":458301,"user":{"displayName":"Vasco","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04380586738115098026"}},"outputId":"7bfe9e36-ea4b-42fe-c947-70d95420a4f6"},"source":["train = ECBMentionsDataset(ecb_input_path, TRAIN)\n","train.parse_data()\n","train_data = train.docs"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing -> 32_10ecbplus.xml\n","Parsing -> 32_1ecbplus.xml\n","Parsing -> 32_11ecbplus.xml\n","Parsing -> 32_1ecb.xml\n","Skip 32_1ecb.xml\n","Parsing -> 32_2ecb.xml\n","Parsing -> 32_3ecb.xml\n","Parsing -> 32_2ecbplus.xml\n","Parsing -> 32_3ecbplus.xml\n","Parsing -> 32_4ecbplus.xml\n","Parsing -> 32_4ecb.xml\n","Parsing -> 32_5ecbplus.xml\n","Parsing -> 32_5ecb.xml\n","Parsing -> 32_7ecbplus.xml\n","Parsing -> 32_7ecb.xml\n","Parsing -> 32_6ecb.xml\n","Parsing -> 32_6ecbplus.xml\n","Parsing -> 32_8ecb.xml\n","Parsing -> 32_8ecbplus.xml\n","Parsing -> 32_9ecbplus.xml\n","3.7% Done parsing topic -> \t 32\n","Parsing -> 33_1ecb.xml\n","Parsing -> 33_10ecbplus.xml\n","Parsing -> 33_11ecbplus.xml\n","Parsing -> 33_3ecb.xml\n","Parsing -> 33_2ecbplus.xml\n","Parsing -> 33_2ecb.xml\n","Parsing -> 33_1ecbplus.xml\n","Parsing -> 33_4ecb.xml\n","Parsing -> 33_4ecbplus.xml\n","Parsing -> 33_3ecbplus.xml\n","Parsing -> 33_8ecbplus.xml\n","Parsing -> 33_9ecbplus.xml\n","Parsing -> 33_5ecb.xml\n","Skip 33_5ecb.xml\n","Parsing -> 33_6ecbplus.xml\n","Parsing -> 33_5ecbplus.xml\n","Parsing -> 33_7ecbplus.xml\n","Skip 33_7ecbplus.xml\n","7.41% Done parsing topic -> \t 33\n","Parsing -> 20_1ecb.xml\n","Parsing -> 20_11ecbplus.xml\n","Parsing -> 20_10ecbplus.xml\n","Parsing -> 20_2ecb.xml\n","Parsing -> 20_1ecbplus.xml\n","Parsing -> 20_3ecb.xml\n","Parsing -> 20_4ecb.xml\n","Parsing -> 20_3ecbplus.xml\n","Skip 20_3ecbplus.xml\n","Parsing -> 20_2ecbplus.xml\n","Parsing -> 20_4ecbplus.xml\n","Parsing -> 20_5ecbplus.xml\n","Parsing -> 20_5ecb.xml\n","Parsing -> 20_6ecbplus.xml\n","Parsing -> 20_9ecbplus.xml\n","Parsing -> 20_7ecbplus.xml\n","Parsing -> 20_8ecbplus.xml\n","11.11% Done parsing topic -> \t 20\n","Parsing -> 29_11ecb.xml\n","Parsing -> 29_10ecb.xml\n","Skip 29_10ecb.xml\n","Parsing -> 29_10ecbplus.xml\n","Parsing -> 29_12ecbplus.xml\n","Parsing -> 29_13ecbplus.xml\n","Parsing -> 29_1ecb.xml\n","Parsing -> 29_11ecbplus.xml\n","Parsing -> 29_2ecbplus.xml\n","Skip 29_2ecbplus.xml\n","Parsing -> 29_2ecb.xml\n","Skip 29_2ecb.xml\n","Parsing -> 29_1ecbplus.xml\n","Parsing -> 29_4ecbplus.xml\n","Parsing -> 29_4ecb.xml\n","Parsing -> 29_3ecbplus.xml\n","Parsing -> 29_3ecb.xml\n","Parsing -> 29_5ecb.xml\n","Parsing -> 29_6ecbplus.xml\n","Parsing -> 29_6ecb.xml\n","Parsing -> 29_5ecbplus.xml\n","Parsing -> 29_7ecbplus.xml\n","Parsing -> 29_8ecbplus.xml\n","Parsing -> 29_9ecb.xml\n","Parsing -> 29_8ecb.xml\n","Parsing -> 29_7ecb.xml\n","Parsing -> 29_9ecbplus.xml\n","14.81% Done parsing topic -> \t 29\n","Parsing -> 9_11ecbplus.xml\n","Parsing -> 9_12ecbplus.xml\n","Parsing -> 9_10ecbplus.xml\n","Parsing -> 9_10ecb.xml\n","Parsing -> 9_1ecb.xml\n","Parsing -> 9_2ecbplus.xml\n","Parsing -> 9_13ecbplus.xml\n","Parsing -> 9_2ecb.xml\n","Parsing -> 9_1ecbplus.xml\n","Parsing -> 9_3ecb.xml\n","Parsing -> 9_3ecbplus.xml\n","Parsing -> 9_4ecb.xml\n","Parsing -> 9_4ecbplus.xml\n","Parsing -> 9_6ecb.xml\n","Parsing -> 9_6ecbplus.xml\n","Parsing -> 9_5ecb.xml\n","Parsing -> 9_5ecbplus.xml\n","Parsing -> 9_8ecb.xml\n","Parsing -> 9_7ecbplus.xml\n","Parsing -> 9_8ecbplus.xml\n","Parsing -> 9_7ecb.xml\n","Parsing -> 9_9ecb.xml\n","Parsing -> 9_9ecbplus.xml\n","18.52% Done parsing topic -> \t 9\n","Parsing -> 27_10ecb.xml\n","Skip 27_10ecb.xml\n","Parsing -> 27_10ecbplus.xml\n","Parsing -> 27_13ecb.xml\n","Parsing -> 27_11ecb.xml\n","Parsing -> 27_12ecb.xml\n","Parsing -> 27_11ecbplus.xml\n","Parsing -> 27_14ecb.xml\n","Parsing -> 27_15ecb.xml\n","Parsing -> 27_16ecb.xml\n","Parsing -> 27_2ecb.xml\n","Parsing -> 27_1ecbplus.xml\n","Parsing -> 27_1ecb.xml\n","Parsing -> 27_17ecb.xml\n","Parsing -> 27_4ecb.xml\n","Parsing -> 27_3ecbplus.xml\n","Parsing -> 27_2ecbplus.xml\n","Parsing -> 27_3ecb.xml\n","Parsing -> 27_4ecbplus.xml\n","Parsing -> 27_6ecb.xml\n","Parsing -> 27_5ecb.xml\n","Parsing -> 27_5ecbplus.xml\n","Parsing -> 27_6ecbplus.xml\n","Parsing -> 27_9ecb.xml\n","Parsing -> 27_8ecbplus.xml\n","Parsing -> 27_7ecb.xml\n","Parsing -> 27_8ecb.xml\n","Skip 27_8ecb.xml\n","Parsing -> 27_7ecbplus.xml\n","Parsing -> 27_9ecbplus.xml\n","22.22% Done parsing topic -> \t 27\n","Parsing -> 7_11ecbplus.xml\n","Parsing -> 7_10ecbplus.xml\n","Parsing -> 7_10ecb.xml\n","Skip 7_10ecb.xml\n","Parsing -> 7_11ecb.xml\n","Parsing -> 7_2ecbplus.xml\n","Parsing -> 7_2ecb.xml\n","Skip 7_2ecb.xml\n","Parsing -> 7_1ecbplus.xml\n","Parsing -> 7_1ecb.xml\n","Skip 7_1ecb.xml\n","Parsing -> 7_4ecbplus.xml\n","Parsing -> 7_3ecbplus.xml\n","Parsing -> 7_3ecb.xml\n","Parsing -> 7_5ecb.xml\n","Skip 7_5ecb.xml\n","Parsing -> 7_5ecbplus.xml\n","Parsing -> 7_6ecb.xml\n","Parsing -> 7_7ecb.xml\n","Skip 7_7ecb.xml\n","Parsing -> 7_6ecbplus.xml\n","Parsing -> 7_9ecb.xml\n","Skip 7_9ecb.xml\n","Parsing -> 7_8ecbplus.xml\n","Parsing -> 7_9ecbplus.xml\n","Parsing -> 7_8ecb.xml\n","Skip 7_8ecb.xml\n","Parsing -> 7_7ecbplus.xml\n","25.93% Done parsing topic -> \t 7\n","Parsing -> 11_1ecb.xml\n","Parsing -> 11_10ecb.xml\n","Parsing -> 11_11ecb.xml\n","Skip 11_11ecb.xml\n","Parsing -> 11_2ecbplus.xml\n","Skip 11_2ecbplus.xml\n","Parsing -> 11_1ecbplus.xml\n","Parsing -> 11_3ecb.xml\n","Skip 11_3ecb.xml\n","Parsing -> 11_2ecb.xml\n","Skip 11_2ecb.xml\n","Parsing -> 11_3ecbplus.xml\n","Parsing -> 11_4ecbplus.xml\n","Parsing -> 11_4ecb.xml\n","Parsing -> 11_5ecb.xml\n","Skip 11_5ecb.xml\n","Parsing -> 11_6ecb.xml\n","Skip 11_6ecb.xml\n","Parsing -> 11_5ecbplus.xml\n","Parsing -> 11_7ecb.xml\n","Parsing -> 11_8ecb.xml\n","Parsing -> 11_9ecb.xml\n","29.63% Done parsing topic -> \t 11\n","Parsing -> 16_11ecbplus.xml\n","Parsing -> 16_10ecbplus.xml\n","Parsing -> 16_1ecb.xml\n","Parsing -> 16_3ecb.xml\n","Parsing -> 16_3ecbplus.xml\n","Parsing -> 16_1ecbplus.xml\n","Parsing -> 16_2ecbplus.xml\n","Skip 16_2ecbplus.xml\n","Parsing -> 16_2ecb.xml\n","Parsing -> 16_4ecbplus.xml\n","Parsing -> 16_5ecbplus.xml\n","Parsing -> 16_6ecbplus.xml\n","Parsing -> 16_7ecbplus.xml\n","Parsing -> 16_8ecbplus.xml\n","Parsing -> 16_9ecbplus.xml\n","33.33% Done parsing topic -> \t 16\n","Parsing -> 28_11ecb.xml\n","Skip 28_11ecb.xml\n","Parsing -> 28_10ecbplus.xml\n","Parsing -> 28_10ecb.xml\n","Skip 28_10ecb.xml\n","Parsing -> 28_13ecb.xml\n","Parsing -> 28_12ecbplus.xml\n","Parsing -> 28_11ecbplus.xml\n","Parsing -> 28_12ecb.xml\n","Parsing -> 28_1ecbplus.xml\n","Parsing -> 28_1ecb.xml\n","Parsing -> 28_2ecb.xml\n","Skip 28_2ecb.xml\n","Parsing -> 28_3ecb.xml\n","Parsing -> 28_4ecb.xml\n","Skip 28_4ecb.xml\n","Parsing -> 28_3ecbplus.xml\n","Parsing -> 28_2ecbplus.xml\n","Parsing -> 28_5ecb.xml\n","Parsing -> 28_4ecbplus.xml\n","Parsing -> 28_5ecbplus.xml\n","Parsing -> 28_6ecbplus.xml\n","Parsing -> 28_6ecb.xml\n","Parsing -> 28_7ecb.xml\n","Parsing -> 28_8ecbplus.xml\n","Parsing -> 28_9ecb.xml\n","Skip 28_9ecb.xml\n","Parsing -> 28_7ecbplus.xml\n","Parsing -> 28_8ecb.xml\n","Parsing -> 28_9ecbplus.xml\n","37.04% Done parsing topic -> \t 28\n","Parsing -> 6_10ecbplus.xml\n","Skip 6_10ecbplus.xml\n","Parsing -> 6_1ecbplus.xml\n","Parsing -> 6_1ecb.xml\n","Skip 6_1ecb.xml\n","Parsing -> 6_11ecbplus.xml\n","Skip 6_11ecbplus.xml\n","Parsing -> 6_3ecbplus.xml\n","Parsing -> 6_2ecbplus.xml\n","Parsing -> 6_2ecb.xml\n","Parsing -> 6_4ecb.xml\n","Skip 6_4ecb.xml\n","Parsing -> 6_3ecb.xml\n","Skip 6_3ecb.xml\n","Parsing -> 6_5ecb.xml\n","Parsing -> 6_6ecb.xml\n","Parsing -> 6_4ecbplus.xml\n","Parsing -> 6_5ecbplus.xml\n","Parsing -> 6_8ecb.xml\n","Parsing -> 6_7ecb.xml\n","Parsing -> 6_6ecbplus.xml\n","Parsing -> 6_7ecbplus.xml\n","Parsing -> 6_8ecbplus.xml\n","Parsing -> 6_9ecbplus.xml\n","Parsing -> 6_9ecb.xml\n","40.74% Done parsing topic -> \t 6\n","Parsing -> 1_11ecbplus.xml\n","Parsing -> 1_10ecbplus.xml\n","Parsing -> 1_10ecb.xml\n","Parsing -> 1_11ecb.xml\n","Skip 1_11ecb.xml\n","Parsing -> 1_13ecb.xml\n","Parsing -> 1_12ecb.xml\n","Parsing -> 1_13ecbplus.xml\n","Parsing -> 1_12ecbplus.xml\n","Parsing -> 1_14ecb.xml\n","Skip 1_14ecb.xml\n","Parsing -> 1_17ecb.xml\n","Parsing -> 1_15ecbplus.xml\n","Parsing -> 1_15ecb.xml\n","Parsing -> 1_14ecbplus.xml\n","Parsing -> 1_16ecbplus.xml\n","Parsing -> 1_17ecbplus.xml\n","Skip 1_17ecbplus.xml\n","Parsing -> 1_18ecbplus.xml\n","Parsing -> 1_18ecb.xml\n","Parsing -> 1_19ecb.xml\n","Skip 1_19ecb.xml\n","Parsing -> 1_19ecbplus.xml\n","Parsing -> 1_1ecb.xml\n","Parsing -> 1_1ecbplus.xml\n","Parsing -> 1_20ecbplus.xml\n","Parsing -> 1_21ecbplus.xml\n","Parsing -> 1_2ecbplus.xml\n","Parsing -> 1_2ecb.xml\n","Parsing -> 1_3ecb.xml\n","Skip 1_3ecb.xml\n","Parsing -> 1_5ecb.xml\n","Parsing -> 1_3ecbplus.xml\n","Parsing -> 1_4ecb.xml\n","Parsing -> 1_4ecbplus.xml\n","Parsing -> 1_5ecbplus.xml\n","Parsing -> 1_7ecbplus.xml\n","Parsing -> 1_6ecb.xml\n","Parsing -> 1_6ecbplus.xml\n","Skip 1_6ecbplus.xml\n","Parsing -> 1_7ecb.xml\n","Parsing -> 1_9ecb.xml\n","Parsing -> 1_8ecbplus.xml\n","Parsing -> 1_8ecb.xml\n","Parsing -> 1_9ecbplus.xml\n","44.44% Done parsing topic -> \t 1\n","Parsing -> 8_10ecbplus.xml\n","Skip 8_10ecbplus.xml\n","Parsing -> 8_11ecbplus.xml\n","Parsing -> 8_2ecbplus.xml\n","Parsing -> 8_1ecb.xml\n","Parsing -> 8_2ecb.xml\n","Parsing -> 8_1ecbplus.xml\n","Parsing -> 8_3ecbplus.xml\n","Parsing -> 8_4ecbplus.xml\n","Parsing -> 8_5ecb.xml\n","Parsing -> 8_3ecb.xml\n","Parsing -> 8_4ecb.xml\n","Parsing -> 8_7ecb.xml\n","Skip 8_7ecb.xml\n","Parsing -> 8_5ecbplus.xml\n","Parsing -> 8_6ecbplus.xml\n","Parsing -> 8_6ecb.xml\n","Skip 8_6ecb.xml\n","Parsing -> 8_8ecb.xml\n","Parsing -> 8_7ecbplus.xml\n","Parsing -> 8_8ecbplus.xml\n","Parsing -> 8_9ecbplus.xml\n","48.15% Done parsing topic -> \t 8\n","Parsing -> 19_11ecb.xml\n","Parsing -> 19_10ecb.xml\n","Parsing -> 19_10ecbplus.xml\n","Parsing -> 19_14ecb.xml\n","Parsing -> 19_12ecb.xml\n","Parsing -> 19_11ecbplus.xml\n","Parsing -> 19_1ecb.xml\n","Parsing -> 19_15ecb.xml\n","Parsing -> 19_1ecbplus.xml\n","Parsing -> 19_2ecbplus.xml\n","Parsing -> 19_2ecb.xml\n","Parsing -> 19_3ecb.xml\n","Parsing -> 19_5ecb.xml\n","Parsing -> 19_3ecbplus.xml\n","Parsing -> 19_4ecbplus.xml\n","Parsing -> 19_4ecb.xml\n","Parsing -> 19_6ecb.xml\n","Parsing -> 19_5ecbplus.xml\n","Parsing -> 19_7ecbplus.xml\n","Parsing -> 19_6ecbplus.xml\n","Parsing -> 19_7ecb.xml\n","Parsing -> 19_8ecb.xml\n","Parsing -> 19_8ecbplus.xml\n","Parsing -> 19_9ecb.xml\n","Parsing -> 19_9ecbplus.xml\n","51.85% Done parsing topic -> \t 19\n","Parsing -> 26_10ecb.xml\n","Parsing -> 26_10ecbplus.xml\n","Parsing -> 26_12ecb.xml\n","Parsing -> 26_13ecb.xml\n","Parsing -> 26_11ecb.xml\n","Parsing -> 26_11ecbplus.xml\n","Parsing -> 26_1ecb.xml\n","Skip 26_1ecb.xml\n","Parsing -> 26_2ecb.xml\n","Parsing -> 26_2ecbplus.xml\n","Parsing -> 26_1ecbplus.xml\n","Parsing -> 26_4ecb.xml\n","Parsing -> 26_3ecb.xml\n","Parsing -> 26_3ecbplus.xml\n","Parsing -> 26_7ecb.xml\n","Parsing -> 26_5ecbplus.xml\n","Parsing -> 26_6ecb.xml\n","Parsing -> 26_5ecb.xml\n","Parsing -> 26_4ecbplus.xml\n","Parsing -> 26_8ecb.xml\n","Parsing -> 26_8ecbplus.xml\n","Parsing -> 26_6ecbplus.xml\n","Parsing -> 26_7ecbplus.xml\n","Parsing -> 26_9ecbplus.xml\n","Parsing -> 26_9ecb.xml\n","55.56% Done parsing topic -> \t 26\n","Parsing -> 10_17ecbplus.xml\n","Parsing -> 10_13ecbplus.xml\n","Parsing -> 10_18ecbplus.xml\n","Parsing -> 10_15ecbplus.xml\n","Parsing -> 10_19ecbplus.xml\n","Parsing -> 10_20ecbplus.xml\n","Parsing -> 10_1ecb.xml\n","Parsing -> 10_1ecbplus.xml\n","Parsing -> 10_21ecbplus.xml\n","Parsing -> 10_2ecbplus.xml\n","Parsing -> 10_4ecb.xml\n","Skip 10_4ecb.xml\n","Parsing -> 10_3ecb.xml\n","Parsing -> 10_3ecbplus.xml\n","Parsing -> 10_2ecb.xml\n","Parsing -> 10_4ecbplus.xml\n","Parsing -> 10_6ecb.xml\n","Parsing -> 10_6ecbplus.xml\n","Parsing -> 10_5ecb.xml\n","Parsing -> 10_7ecb.xml\n","Parsing -> 10_8ecb.xml\n","Parsing -> 10_9ecbplus.xml\n","59.26% Done parsing topic -> \t 10\n","Parsing -> 31_10ecbplus.xml\n","Parsing -> 31_10ecb.xml\n","Parsing -> 31_11ecb.xml\n","Skip 31_11ecb.xml\n","Parsing -> 31_11ecbplus.xml\n","Skip 31_11ecbplus.xml\n","Parsing -> 31_14ecb.xml\n","Skip 31_14ecb.xml\n","Parsing -> 31_12ecb.xml\n","Parsing -> 31_13ecb.xml\n","Parsing -> 31_2ecb.xml\n","Parsing -> 31_1ecb.xml\n","Parsing -> 31_1ecbplus.xml\n","Parsing -> 31_3ecb.xml\n","Skip 31_3ecb.xml\n","Parsing -> 31_3ecbplus.xml\n","Parsing -> 31_2ecbplus.xml\n","Parsing -> 31_4ecb.xml\n","Skip 31_4ecb.xml\n","Parsing -> 31_5ecbplus.xml\n","Skip 31_5ecbplus.xml\n","Parsing -> 31_4ecbplus.xml\n","Parsing -> 31_6ecb.xml\n","Skip 31_6ecb.xml\n","Parsing -> 31_5ecb.xml\n","Skip 31_5ecb.xml\n","Parsing -> 31_8ecb.xml\n","Parsing -> 31_6ecbplus.xml\n","Parsing -> 31_7ecbplus.xml\n","Parsing -> 31_7ecb.xml\n","Parsing -> 31_9ecb.xml\n","Parsing -> 31_8ecbplus.xml\n","Parsing -> 31_9ecbplus.xml\n","62.96% Done parsing topic -> \t 31\n","Parsing -> 30_11ecb.xml\n","Parsing -> 30_10ecbplus.xml\n","Parsing -> 30_10ecb.xml\n","Parsing -> 30_11ecbplus.xml\n","Parsing -> 30_12ecbplus.xml\n","Parsing -> 30_12ecb.xml\n","Parsing -> 30_13ecb.xml\n","Parsing -> 30_14ecb.xml\n","Skip 30_14ecb.xml\n","Parsing -> 30_1ecbplus.xml\n","Parsing -> 30_13ecbplus.xml\n","Parsing -> 30_1ecb.xml\n","Parsing -> 30_4ecb.xml\n","Parsing -> 30_2ecbplus.xml\n","Parsing -> 30_3ecbplus.xml\n","Parsing -> 30_2ecb.xml\n","Skip 30_2ecb.xml\n","Parsing -> 30_3ecb.xml\n","Parsing -> 30_5ecb.xml\n","Parsing -> 30_4ecbplus.xml\n","Parsing -> 30_6ecb.xml\n","Parsing -> 30_5ecbplus.xml\n","Parsing -> 30_6ecbplus.xml\n","Parsing -> 30_8ecb.xml\n","Parsing -> 30_7ecb.xml\n","Parsing -> 30_7ecbplus.xml\n","Parsing -> 30_9ecbplus.xml\n","Parsing -> 30_8ecbplus.xml\n","Parsing -> 30_9ecb.xml\n","66.67% Done parsing topic -> \t 30\n","Parsing -> 24_10ecb.xml\n","Skip 24_10ecb.xml\n","Parsing -> 24_11ecb.xml\n","Parsing -> 24_12ecb.xml\n","Skip 24_12ecb.xml\n","Parsing -> 24_11ecbplus.xml\n","Parsing -> 24_13ecb.xml\n","Parsing -> 24_10ecbplus.xml\n","Parsing -> 24_1ecb.xml\n","Parsing -> 24_14ecb.xml\n","Parsing -> 24_15ecb.xml\n","Parsing -> 24_1ecbplus.xml\n","Parsing -> 24_2ecbplus.xml\n","Parsing -> 24_2ecb.xml\n","Parsing -> 24_4ecb.xml\n","Parsing -> 24_3ecb.xml\n","Parsing -> 24_3ecbplus.xml\n","Parsing -> 24_4ecbplus.xml\n","Parsing -> 24_5ecb.xml\n","Parsing -> 24_5ecbplus.xml\n","Parsing -> 24_6ecb.xml\n","Parsing -> 24_7ecbplus.xml\n","Parsing -> 24_7ecb.xml\n","Parsing -> 24_8ecb.xml\n","Skip 24_8ecb.xml\n","Parsing -> 24_6ecbplus.xml\n","Parsing -> 24_9ecb.xml\n","Parsing -> 24_9ecbplus.xml\n","Parsing -> 24_8ecbplus.xml\n","70.37% Done parsing topic -> \t 24\n","Parsing -> 3_10ecbplus.xml\n","Parsing -> 3_11ecbplus.xml\n","Parsing -> 3_3ecb.xml\n","Parsing -> 3_2ecb.xml\n","Parsing -> 3_1ecbplus.xml\n","Skip 3_1ecbplus.xml\n","Parsing -> 3_1ecb.xml\n","Parsing -> 3_2ecbplus.xml\n","Skip 3_2ecbplus.xml\n","Parsing -> 3_4ecb.xml\n","Parsing -> 3_3ecbplus.xml\n","Parsing -> 3_5ecb.xml\n","Parsing -> 3_4ecbplus.xml\n","Skip 3_4ecbplus.xml\n","Parsing -> 3_6ecbplus.xml\n","Parsing -> 3_7ecb.xml\n","Parsing -> 3_5ecbplus.xml\n","Parsing -> 3_7ecbplus.xml\n","Parsing -> 3_6ecb.xml\n","Parsing -> 3_8ecbplus.xml\n","Parsing -> 3_9ecbplus.xml\n","Parsing -> 3_9ecb.xml\n","Parsing -> 3_8ecb.xml\n","74.07% Done parsing topic -> \t 3\n","Parsing -> 4_11ecb.xml\n","Skip 4_11ecb.xml\n","Parsing -> 4_10ecb.xml\n","Skip 4_10ecb.xml\n","Parsing -> 4_10ecbplus.xml\n","Parsing -> 4_11ecbplus.xml\n","Parsing -> 4_1ecbplus.xml\n","Parsing -> 4_12ecb.xml\n","Skip 4_12ecb.xml\n","Parsing -> 4_14ecb.xml\n","Parsing -> 4_13ecb.xml\n","Parsing -> 4_1ecb.xml\n","Skip 4_1ecb.xml\n","Parsing -> 4_2ecbplus.xml\n","Parsing -> 4_3ecb.xml\n","Skip 4_3ecb.xml\n","Parsing -> 4_2ecb.xml\n","Skip 4_2ecb.xml\n","Parsing -> 4_3ecbplus.xml\n","Parsing -> 4_5ecbplus.xml\n","Parsing -> 4_4ecbplus.xml\n","Parsing -> 4_4ecb.xml\n","Skip 4_4ecb.xml\n","Parsing -> 4_5ecb.xml\n","Parsing -> 4_6ecbplus.xml\n","Parsing -> 4_7ecb.xml\n","Skip 4_7ecb.xml\n","Parsing -> 4_8ecbplus.xml\n","Parsing -> 4_6ecb.xml\n","Parsing -> 4_8ecb.xml\n","Skip 4_8ecb.xml\n","Parsing -> 4_9ecbplus.xml\n","Parsing -> 4_9ecb.xml\n","77.78% Done parsing topic -> \t 4\n","Parsing -> 13_10ecb.xml\n","Parsing -> 13_11ecb.xml\n","Skip 13_11ecb.xml\n","Parsing -> 13_10ecbplus.xml\n","Parsing -> 13_13ecb.xml\n","Parsing -> 13_12ecb.xml\n","Parsing -> 13_11ecbplus.xml\n","Skip 13_11ecbplus.xml\n","Parsing -> 13_12ecbplus.xml\n","Parsing -> 13_15ecb.xml\n","Parsing -> 13_13ecbplus.xml\n","Parsing -> 13_14ecb.xml\n","Skip 13_14ecb.xml\n","Parsing -> 13_14ecbplus.xml\n","Parsing -> 13_17ecb.xml\n","Parsing -> 13_18ecb.xml\n","Parsing -> 13_16ecb.xml\n","Parsing -> 13_19ecb.xml\n","Parsing -> 13_1ecb.xml\n","Parsing -> 13_20ecb.xml\n","Skip 13_20ecb.xml\n","Parsing -> 13_21ecb.xml\n","Parsing -> 13_22ecb.xml\n","Parsing -> 13_3ecb.xml\n","Parsing -> 13_2ecb.xml\n","Parsing -> 13_4ecb.xml\n","Parsing -> 13_5ecb.xml\n","Parsing -> 13_4ecbplus.xml\n","Parsing -> 13_6ecb.xml\n","Skip 13_6ecb.xml\n","Parsing -> 13_5ecbplus.xml\n","Parsing -> 13_6ecbplus.xml\n","Parsing -> 13_7ecb.xml\n","Parsing -> 13_8ecb.xml\n","Skip 13_8ecb.xml\n","Parsing -> 13_7ecbplus.xml\n","Skip 13_7ecbplus.xml\n","Parsing -> 13_9ecb.xml\n","Skip 13_9ecb.xml\n","Parsing -> 13_8ecbplus.xml\n","Skip 13_8ecbplus.xml\n","Parsing -> 13_9ecbplus.xml\n","81.48% Done parsing topic -> \t 13\n","Parsing -> 14_10ecbplus.xml\n","Parsing -> 14_10ecb.xml\n","Parsing -> 14_11ecbplus.xml\n","Parsing -> 14_2ecb.xml\n","Skip 14_2ecb.xml\n","Parsing -> 14_1ecbplus.xml\n","Parsing -> 14_1ecb.xml\n","Skip 14_1ecb.xml\n","Parsing -> 14_3ecb.xml\n","Skip 14_3ecb.xml\n","Parsing -> 14_3ecbplus.xml\n","Parsing -> 14_4ecb.xml\n","Skip 14_4ecb.xml\n","Parsing -> 14_4ecbplus.xml\n","Parsing -> 14_2ecbplus.xml\n","Parsing -> 14_6ecb.xml\n","Skip 14_6ecb.xml\n","Parsing -> 14_5ecbplus.xml\n","Parsing -> 14_6ecbplus.xml\n","Parsing -> 14_5ecb.xml\n","Parsing -> 14_7ecb.xml\n","Parsing -> 14_9ecb.xml\n","Skip 14_9ecb.xml\n","Parsing -> 14_8ecb.xml\n","Parsing -> 14_8ecbplus.xml\n","Parsing -> 14_7ecbplus.xml\n","Parsing -> 14_9ecbplus.xml\n","85.19% Done parsing topic -> \t 14\n","Parsing -> 25_10ecb.xml\n","Parsing -> 25_13ecb.xml\n","Skip 25_13ecb.xml\n","Parsing -> 25_12ecb.xml\n","Skip 25_12ecb.xml\n","Parsing -> 25_11ecbplus.xml\n","Parsing -> 25_11ecb.xml\n","Skip 25_11ecb.xml\n","Parsing -> 25_10ecbplus.xml\n","Parsing -> 25_15ecb.xml\n","Parsing -> 25_14ecbplus.xml\n","Parsing -> 25_14ecb.xml\n","Parsing -> 25_13ecbplus.xml\n","Parsing -> 25_15ecbplus.xml\n","Parsing -> 25_3ecb.xml\n","Parsing -> 25_1ecb.xml\n","Parsing -> 25_2ecb.xml\n","Parsing -> 25_5ecb.xml\n","Parsing -> 25_4ecb.xml\n","Parsing -> 25_5ecbplus.xml\n","Parsing -> 25_3ecbplus.xml\n","Parsing -> 25_6ecb.xml\n","Parsing -> 25_7ecb.xml\n","Parsing -> 25_8ecb.xml\n","Parsing -> 25_6ecbplus.xml\n","Parsing -> 25_7ecbplus.xml\n","Parsing -> 25_8ecbplus.xml\n","Parsing -> 25_9ecb.xml\n","Parsing -> 25_9ecbplus.xml\n","88.89% Done parsing topic -> \t 25\n","Parsing -> 22_10ecbplus.xml\n","Parsing -> 22_11ecbplus.xml\n","Parsing -> 22_13ecbplus.xml\n","Parsing -> 22_12ecbplus.xml\n","Parsing -> 22_14ecbplus.xml\n","Parsing -> 22_2ecb.xml\n","Parsing -> 22_1ecbplus.xml\n","Parsing -> 22_2ecbplus.xml\n","Parsing -> 22_1ecb.xml\n","Parsing -> 22_3ecbplus.xml\n","Parsing -> 22_4ecbplus.xml\n","Parsing -> 22_3ecb.xml\n","Parsing -> 22_4ecb.xml\n","Parsing -> 22_5ecbplus.xml\n","Parsing -> 22_6ecbplus.xml\n","Parsing -> 22_5ecb.xml\n","Parsing -> 22_6ecb.xml\n","Skip 22_6ecb.xml\n","Parsing -> 22_8ecb.xml\n","Parsing -> 22_7ecbplus.xml\n","Parsing -> 22_7ecb.xml\n","Parsing -> 22_9ecb.xml\n","Skip 22_9ecb.xml\n","Parsing -> 22_9ecbplus.xml\n","Parsing -> 22_8ecbplus.xml\n","92.59% Done parsing topic -> \t 22\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUXOXN1qQEdF","executionInfo":{"status":"ok","timestamp":1638687394797,"user_tz":300,"elapsed":519,"user":{"displayName":"Tim Wu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Mbx_gve9TlcvS-Od6nAnJwp6jEIt8BeHBKSp=s64","userId":"18355578200475546173"}},"outputId":"d6dacf0c-2fe5-4b7b-e468-08e1deb525e9"},"source":["print(len(train_data))\n","\n","file_path = ecb_path + \"processed/dev_with_sent_idx.json\"\n","with open(file_path, 'w') as f:\n","    json.dump(train_data, f)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["558\n"]}]},{"cell_type":"markdown","metadata":{"id":"e6rNsdGAbaQY"},"source":[""]}]}